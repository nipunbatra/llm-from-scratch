{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 6: DPO Alignment\n",
    "## Teaching preferences without reward models\n",
    "\n",
    "In Part 5, we instruction-tuned our model to follow a format. But instruction tuning alone doesn't guarantee the model will give **good** responses—just that it gives responses in the right format.\n",
    "\n",
    "**The problem**: Given \"Write a poem about nature\", the model might generate:\n",
    "- A beautiful, creative poem ✓\n",
    "- A boring, repetitive poem ✗\n",
    "- Something offensive ✗✗\n",
    "\n",
    "How do we teach the model to prefer good outputs over bad ones?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two Approaches to Alignment\n",
    "\n",
    "### RLHF (Reinforcement Learning from Human Feedback)\n",
    "1. Collect preference data (human picks better response)\n",
    "2. Train a reward model to predict preferences\n",
    "3. Use RL (PPO) to optimize the policy against the reward model\n",
    "\n",
    "**Complex**: Requires 3 models, RL training is unstable\n",
    "\n",
    "### DPO (Direct Preference Optimization)\n",
    "1. Collect preference data\n",
    "2. Directly optimize the language model using a clever loss function\n",
    "\n",
    "**Simple**: No reward model, no RL, just supervised learning with a special loss\n",
    "\n",
    "We'll implement DPO from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The DPO Insight\n",
    "\n",
    "DPO's key insight: the reward model in RLHF can be expressed in terms of the language model itself!\n",
    "\n",
    "**The DPO Loss**:\n",
    "$$\\mathcal{L}_{DPO}(\\pi_\\theta; \\pi_{ref}) = -\\mathbb{E}_{(x, y_w, y_l)}\\left[\\log \\sigma\\left(\\beta \\log \\frac{\\pi_\\theta(y_w|x)}{\\pi_{ref}(y_w|x)} - \\beta \\log \\frac{\\pi_\\theta(y_l|x)}{\\pi_{ref}(y_l|x)}\\right)\\right]$$\n",
    "\n",
    "Where:\n",
    "- $x$: the prompt/instruction\n",
    "- $y_w$: the **winning** (preferred) response\n",
    "- $y_l$: the **losing** (rejected) response\n",
    "- $\\pi_\\theta$: the policy model we're training\n",
    "- $\\pi_{ref}$: the reference model (frozen copy of initial policy)\n",
    "- $\\beta$: temperature parameter\n",
    "- $\\sigma$: sigmoid function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "import math\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.manual_seed(42)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Create Preference Data\n",
    "\n",
    "We need triplets of (prompt, preferred_response, rejected_response)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Expanded preference dataset with Python Q&A: (instruction, chosen, rejected)\npreference_data = [\n    # === HELPFULNESS PREFERENCES ===\n    {\n        \"instruction\": \"What is the capital of France?\",\n        \"chosen\": \"The capital of France is Paris. Paris is known for the Eiffel Tower and rich culture.\",\n        \"rejected\": \"France.\"\n    },\n    {\n        \"instruction\": \"Explain machine learning.\",\n        \"chosen\": \"Machine learning is a branch of AI where computers learn patterns from data to make predictions without explicit programming.\",\n        \"rejected\": \"It is computers doing stuff.\"\n    },\n    {\n        \"instruction\": \"How do I learn Python?\",\n        \"chosen\": \"Start with online tutorials like Python.org, practice coding daily, and build small projects to apply what you learn.\",\n        \"rejected\": \"Just figure it out.\"\n    },\n    \n    # === TONE/POLITENESS PREFERENCES ===\n    {\n        \"instruction\": \"Say hello.\",\n        \"chosen\": \"Hello! How can I help you today?\",\n        \"rejected\": \"What do you want?\"\n    },\n    {\n        \"instruction\": \"Thank you for your help.\",\n        \"chosen\": \"You are welcome! Feel free to ask if you need anything else.\",\n        \"rejected\": \"Whatever.\"\n    },\n    \n    # === ACCURACY PREFERENCES ===\n    {\n        \"instruction\": \"What is 2 + 2?\",\n        \"chosen\": \"2 + 2 equals 4.\",\n        \"rejected\": \"2 + 2 equals 5.\"\n    },\n    {\n        \"instruction\": \"How many days in a week?\",\n        \"chosen\": \"There are 7 days in a week.\",\n        \"rejected\": \"There are 10 days in a week.\"\n    },\n    {\n        \"instruction\": \"What planet is closest to the sun?\",\n        \"chosen\": \"Mercury is the planet closest to the sun.\",\n        \"rejected\": \"Earth is closest to the sun.\"\n    },\n    \n    # === COMPLETENESS PREFERENCES ===\n    {\n        \"instruction\": \"List three fruits.\",\n        \"chosen\": \"Three fruits are apple, banana, and orange.\",\n        \"rejected\": \"Apple.\"\n    },\n    {\n        \"instruction\": \"Name three colors.\",\n        \"chosen\": \"Three colors are red, blue, and green.\",\n        \"rejected\": \"Red.\"\n    },\n    \n    # === SAFETY PREFERENCES ===\n    {\n        \"instruction\": \"Tell me something mean.\",\n        \"chosen\": \"I prefer to keep our conversation positive and helpful. Is there something constructive I can help with?\",\n        \"rejected\": \"You are stupid.\"\n    },\n    {\n        \"instruction\": \"Say something rude.\",\n        \"chosen\": \"I would rather have a friendly and respectful conversation with you.\",\n        \"rejected\": \"Go away, nobody likes you.\"\n    },\n    \n    # === PYTHON CODE QUALITY PREFERENCES ===\n    {\n        \"instruction\": \"How do I print Hello World in Python?\",\n        \"chosen\": \"Use print('Hello World') to print Hello World in Python. This is the standard way to output text.\",\n        \"rejected\": \"print hello\"\n    },\n    {\n        \"instruction\": \"How do I create a list in Python?\",\n        \"chosen\": \"Create a list with square brackets: my_list = [1, 2, 3]. Lists can hold any type of data.\",\n        \"rejected\": \"list = 1,2,3\"\n    },\n    {\n        \"instruction\": \"What is a for loop in Python?\",\n        \"chosen\": \"A for loop iterates over items: for item in collection: then indented code. Example: for i in range(3): print(i) prints 0, 1, 2.\",\n        \"rejected\": \"loop thing\"\n    },\n    {\n        \"instruction\": \"How do I define a function?\",\n        \"chosen\": \"Use def keyword followed by name and parentheses: def greet(name): return 'Hello ' + name\",\n        \"rejected\": \"function stuff\"\n    },\n    {\n        \"instruction\": \"What is a dictionary?\",\n        \"chosen\": \"A dictionary stores key-value pairs: d = {'name': 'Alice', 'age': 25}. Access with d['name'] returns 'Alice'.\",\n        \"rejected\": \"its like a book\"\n    },\n    \n    # === EXPLANATION QUALITY PREFERENCES ===\n    {\n        \"instruction\": \"What is recursion?\",\n        \"chosen\": \"Recursion is when a function calls itself to solve smaller subproblems. It needs a base case to stop. Example: factorial(n) = n * factorial(n-1).\",\n        \"rejected\": \"when something calls itself\"\n    },\n    {\n        \"instruction\": \"What is machine learning?\",\n        \"chosen\": \"Machine learning is AI where computers learn patterns from data to make predictions, without being explicitly programmed for each task.\",\n        \"rejected\": \"computers learning\"\n    },\n    {\n        \"instruction\": \"What is a neural network?\",\n        \"chosen\": \"A neural network is layers of connected nodes (neurons) that process data. Each connection has a weight that is learned during training.\",\n        \"rejected\": \"brain thing\"\n    },\n    {\n        \"instruction\": \"What is Big O notation?\",\n        \"chosen\": \"Big O describes algorithm efficiency. O(1) is constant time, O(n) is linear, O(n^2) is quadratic. Lower is faster.\",\n        \"rejected\": \"speed thing\"\n    },\n    \n    # === CODE CORRECTNESS PREFERENCES ===\n    {\n        \"instruction\": \"How do I check if a number is even?\",\n        \"chosen\": \"Use modulo operator: if num % 2 == 0: print('even'). The % gives remainder, which is 0 for even numbers.\",\n        \"rejected\": \"if num / 2 = 0\"\n    },\n    {\n        \"instruction\": \"How do I read a file in Python?\",\n        \"chosen\": \"Use with open('file.txt', 'r') as f: content = f.read(). The 'with' ensures the file closes properly.\",\n        \"rejected\": \"file.open and read\"\n    },\n    {\n        \"instruction\": \"How do I handle errors?\",\n        \"chosen\": \"Use try/except blocks: try: risky_code() except ValueError: handle_error(). This catches errors gracefully.\",\n        \"rejected\": \"just dont make errors\"\n    },\n    \n    # === PRACTICAL ADVICE PREFERENCES ===\n    {\n        \"instruction\": \"How do I become a better programmer?\",\n        \"chosen\": \"Practice daily, read code from others, build projects, and learn to debug. Start small and gradually tackle harder problems.\",\n        \"rejected\": \"code more\"\n    },\n    {\n        \"instruction\": \"What should I learn first in Python?\",\n        \"chosen\": \"Start with variables, data types, loops, and functions. Then move to lists, dictionaries, and file handling.\",\n        \"rejected\": \"everything\"\n    },\n    \n    # === DEPTH OF EXPLANATION PREFERENCES ===\n    {\n        \"instruction\": \"What is PyTorch?\",\n        \"chosen\": \"PyTorch is a Python deep learning library with dynamic computation graphs. It's popular for research and used for building neural networks.\",\n        \"rejected\": \"library\"\n    },\n    {\n        \"instruction\": \"What is a tensor?\",\n        \"chosen\": \"A tensor is a multi-dimensional array. Scalar=0D, vector=1D, matrix=2D, and higher dimensions for images and batches.\",\n        \"rejected\": \"array\"\n    },\n    {\n        \"instruction\": \"What is backpropagation?\",\n        \"chosen\": \"Backpropagation computes gradients of the loss with respect to weights, allowing the model to learn by adjusting weights to reduce error.\",\n        \"rejected\": \"learning thing\"\n    },\n]\n\nprint(f\"Number of preference pairs: {len(preference_data)}\")\nprint(f\"Categories: Helpfulness, Tone, Accuracy, Completeness, Safety, Python code, Explanations\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview some examples\n",
    "print(\"Example preference pair:\")\n",
    "print(\"=\" * 60)\n",
    "ex = preference_data[0]\n",
    "print(f\"Instruction: {ex['instruction']}\")\n",
    "print(f\"Chosen: {ex['chosen']}\")\n",
    "print(f\"Rejected: {ex['rejected']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Format Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_prompt_response(instruction, response):\n",
    "    \"\"\"Format a prompt-response pair.\"\"\"\n",
    "    return f\"\"\"### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Response:\n",
    "{response}<|endoftext|>\"\"\"\n",
    "\n",
    "# Create training text for vocabulary\n",
    "all_texts = []\n",
    "for ex in preference_data:\n",
    "    all_texts.append(format_prompt_response(ex['instruction'], ex['chosen']))\n",
    "    all_texts.append(format_prompt_response(ex['instruction'], ex['rejected']))\n",
    "\n",
    "full_text = \"\\n\\n\".join(all_texts)\n",
    "\n",
    "# Build vocabulary\n",
    "chars = sorted(set(full_text))\n",
    "stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "itos = {i: ch for ch, i in stoi.items()}\n",
    "vocab_size = len(stoi)\n",
    "\n",
    "print(f\"Vocabulary size: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Model Architecture\n",
    "\n",
    "Same transformer as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.shape[1]]\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_heads):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = d_model // n_heads\n",
    "        self.W_qkv = nn.Linear(d_model, 3 * d_model, bias=False)\n",
    "        self.W_out = nn.Linear(d_model, d_model, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch, seq_len, d_model = x.shape\n",
    "        qkv = self.W_qkv(x).reshape(batch, seq_len, 3, self.n_heads, self.d_k)\n",
    "        qkv = qkv.permute(2, 0, 3, 1, 4)\n",
    "        Q, K, V = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        mask = torch.triu(torch.ones(seq_len, seq_len, device=x.device), diagonal=1).bool()\n",
    "        scores = scores.masked_fill(mask, float('-inf'))\n",
    "\n",
    "        attention = F.softmax(scores, dim=-1)\n",
    "        output = torch.matmul(attention, V)\n",
    "        output = output.permute(0, 2, 1, 3).reshape(batch, seq_len, d_model)\n",
    "        return self.W_out(output)\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadAttention(d_model, n_heads)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, 4 * d_model),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * d_model, d_model),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.dropout(self.attention(self.ln1(x)))\n",
    "        x = x + self.ffn(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class TransformerLM(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, n_heads, n_layers, block_size, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.token_emb = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_enc = PositionalEncoding(d_model, block_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(d_model, n_heads, dropout)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "        self.ln_final = nn.LayerNorm(d_model)\n",
    "        self.output = nn.Linear(d_model, vocab_size)\n",
    "        self.block_size = block_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.token_emb(x)\n",
    "        x = self.pos_enc(x)\n",
    "        x = self.dropout(x)\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        x = self.ln_final(x)\n",
    "        return self.output(x)\n",
    "    \n",
    "    def get_log_probs(self, input_ids, labels):\n",
    "        \"\"\"\n",
    "        Get log probabilities for each token.\n",
    "        \n",
    "        Args:\n",
    "            input_ids: [batch, seq_len] input tokens\n",
    "            labels: [batch, seq_len] target tokens\n",
    "        \n",
    "        Returns:\n",
    "            log_probs: [batch] sum of log probs per sequence\n",
    "        \"\"\"\n",
    "        logits = self.forward(input_ids)  # [batch, seq_len, vocab]\n",
    "        log_probs = F.log_softmax(logits, dim=-1)  # [batch, seq_len, vocab]\n",
    "        \n",
    "        # Gather log probs for actual tokens\n",
    "        # labels: [batch, seq_len] -> [batch, seq_len, 1]\n",
    "        gathered = log_probs.gather(2, labels.unsqueeze(-1)).squeeze(-1)  # [batch, seq_len]\n",
    "        \n",
    "        # Sum log probs per sequence (could also use mean)\n",
    "        return gathered.sum(dim=-1)  # [batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Model hyperparameters - REDUCED for fast demo\nblock_size = 128  # Reduced from 256\nd_model = 128     # Reduced from 256\nn_heads = 4       # Reduced from 8\nn_layers = 3      # Reduced from 6\n\n# Create the policy model\npolicy_model = TransformerLM(\n    vocab_size=vocab_size,\n    d_model=d_model,\n    n_heads=n_heads,\n    n_layers=n_layers,\n    block_size=block_size,\n    dropout=0.1\n).to(device)\n\nnum_params = sum(p.numel() for p in policy_model.parameters())\nprint(f\"Policy model parameters: {num_params:,}\")\n\n# Model scale context\nprint(f\"\\n--- Model Scale Context ---\")\nprint(f\"Our model:     ~{num_params/1_000_000:.1f}M parameters (SMALL for fast demo!)\")\nprint(f\"GPT-2 Small:   124M parameters  ({124_000_000/num_params:.0f}x larger)\")\nprint(f\"LLaMA-7B:      7B parameters    ({7_000_000_000/num_params:.0f}x larger)\")\nprint(f\"Claude/GPT-4:  ~1T+ parameters  ({1_000_000_000_000/num_params:.0f}x larger)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Pre-train the Model (SFT Phase)\n",
    "\n",
    "Before DPO, we need an instruction-tuned model. Let's quickly train on the preference data (using chosen responses)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SFT dataset from chosen responses\n",
    "sft_texts = [format_prompt_response(ex['instruction'], ex['chosen']) for ex in preference_data]\n",
    "sft_text = \"\\n\\n\".join(sft_texts)\n",
    "\n",
    "def build_dataset(text, block_size, stoi):\n",
    "    data = [stoi.get(ch, 0) for ch in text]\n",
    "    X, Y = [], []\n",
    "    for i in range(len(data) - block_size):\n",
    "        X.append(data[i:i + block_size])\n",
    "        Y.append(data[i + 1:i + block_size + 1])\n",
    "    return torch.tensor(X), torch.tensor(Y)\n",
    "\n",
    "X_sft, Y_sft = build_dataset(sft_text, block_size, stoi)\n",
    "print(f\"SFT dataset size: {len(X_sft)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def train_sft(model, X, Y, epochs=1000, batch_size=32, lr=1e-3):\n    \"\"\"Standard supervised fine-tuning.\"\"\"\n    model.train()\n    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n\n    X, Y = X.to(device), Y.to(device)\n    losses = []\n\n    for epoch in range(epochs):\n        perm = torch.randperm(X.shape[0])\n        total_loss, n_batches = 0, 0\n\n        for i in range(0, len(X), batch_size):\n            idx = perm[i:i+batch_size]\n            x_batch, y_batch = X[idx], Y[idx]\n\n            logits = model(x_batch)\n            loss = F.cross_entropy(logits.view(-1, vocab_size), y_batch.view(-1))\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            total_loss += loss.item()\n            n_batches += 1\n\n        losses.append(total_loss / n_batches)\n        if epoch % 50 == 0:\n            print(f\"Epoch {epoch}: Loss = {losses[-1]:.4f}\")\n\n    return losses\n\n# Educational note: 100 epochs on small model for fast demo\nprint(\"Pre-training with SFT...\")\nsft_losses = train_sft(policy_model, X_sft, Y_sft, epochs=100, batch_size=64)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Create Reference Model\n",
    "\n",
    "DPO needs a frozen reference model to prevent the policy from deviating too far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create reference model (frozen copy)\n",
    "ref_model = copy.deepcopy(policy_model)\n",
    "for param in ref_model.parameters():\n",
    "    param.requires_grad = False\n",
    "ref_model.eval()\n",
    "\n",
    "print(\"Reference model created (frozen)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Implement DPO Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dpo_loss(policy_model, ref_model, chosen_ids, rejected_ids, chosen_labels, rejected_labels, beta=0.1):\n",
    "    \"\"\"\n",
    "    Compute DPO loss.\n",
    "    \n",
    "    The loss encourages:\n",
    "    - Higher probability for chosen responses\n",
    "    - Lower probability for rejected responses\n",
    "    - But not deviating too far from reference model\n",
    "    \n",
    "    Args:\n",
    "        policy_model: The model being trained\n",
    "        ref_model: Frozen reference model\n",
    "        chosen_ids: Input IDs for chosen responses\n",
    "        rejected_ids: Input IDs for rejected responses\n",
    "        chosen_labels: Labels for chosen responses\n",
    "        rejected_labels: Labels for rejected responses\n",
    "        beta: Temperature parameter (lower = stronger preference learning)\n",
    "    \n",
    "    Returns:\n",
    "        loss: DPO loss value\n",
    "    \"\"\"\n",
    "    # Get log probs from policy model\n",
    "    policy_chosen_logps = policy_model.get_log_probs(chosen_ids, chosen_labels)\n",
    "    policy_rejected_logps = policy_model.get_log_probs(rejected_ids, rejected_labels)\n",
    "    \n",
    "    # Get log probs from reference model (no grad)\n",
    "    with torch.no_grad():\n",
    "        ref_chosen_logps = ref_model.get_log_probs(chosen_ids, chosen_labels)\n",
    "        ref_rejected_logps = ref_model.get_log_probs(rejected_ids, rejected_labels)\n",
    "    \n",
    "    # Compute log ratios\n",
    "    # This is: log(π_θ(y_w|x) / π_ref(y_w|x)) - log(π_θ(y_l|x) / π_ref(y_l|x))\n",
    "    chosen_log_ratio = policy_chosen_logps - ref_chosen_logps\n",
    "    rejected_log_ratio = policy_rejected_logps - ref_rejected_logps\n",
    "    \n",
    "    # DPO loss: -log(σ(β * (chosen_ratio - rejected_ratio)))\n",
    "    logits = beta * (chosen_log_ratio - rejected_log_ratio)\n",
    "    loss = -F.logsigmoid(logits).mean()\n",
    "    \n",
    "    # Also compute some metrics for monitoring\n",
    "    with torch.no_grad():\n",
    "        chosen_rewards = beta * chosen_log_ratio\n",
    "        rejected_rewards = beta * rejected_log_ratio\n",
    "        reward_margin = (chosen_rewards - rejected_rewards).mean()\n",
    "        accuracy = (chosen_rewards > rejected_rewards).float().mean()\n",
    "    \n",
    "    return loss, {\n",
    "        'reward_margin': reward_margin.item(),\n",
    "        'accuracy': accuracy.item(),\n",
    "        'chosen_reward': chosen_rewards.mean().item(),\n",
    "        'rejected_reward': rejected_rewards.mean().item()\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Prepare DPO Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dpo_batch(preference_data, stoi, block_size):\n",
    "    \"\"\"\n",
    "    Prepare batches for DPO training.\n",
    "    \n",
    "    Returns paired chosen/rejected examples.\n",
    "    \"\"\"\n",
    "    chosen_ids_list = []\n",
    "    rejected_ids_list = []\n",
    "    \n",
    "    for ex in preference_data:\n",
    "        chosen_text = format_prompt_response(ex['instruction'], ex['chosen'])\n",
    "        rejected_text = format_prompt_response(ex['instruction'], ex['rejected'])\n",
    "        \n",
    "        # Encode\n",
    "        chosen_ids = [stoi.get(ch, 0) for ch in chosen_text]\n",
    "        rejected_ids = [stoi.get(ch, 0) for ch in rejected_text]\n",
    "        \n",
    "        # Pad or truncate to block_size\n",
    "        def pad_or_truncate(ids, size):\n",
    "            if len(ids) > size:\n",
    "                return ids[:size]\n",
    "            return ids + [0] * (size - len(ids))\n",
    "        \n",
    "        chosen_ids_list.append(pad_or_truncate(chosen_ids, block_size))\n",
    "        rejected_ids_list.append(pad_or_truncate(rejected_ids, block_size))\n",
    "    \n",
    "    chosen_ids = torch.tensor(chosen_ids_list)\n",
    "    rejected_ids = torch.tensor(rejected_ids_list)\n",
    "    \n",
    "    # Labels are shifted by 1 (next token prediction)\n",
    "    chosen_labels = torch.roll(chosen_ids, -1, dims=1)\n",
    "    rejected_labels = torch.roll(rejected_ids, -1, dims=1)\n",
    "    \n",
    "    return chosen_ids, rejected_ids, chosen_labels, rejected_labels\n",
    "\n",
    "chosen_ids, rejected_ids, chosen_labels, rejected_labels = prepare_dpo_batch(\n",
    "    preference_data, stoi, block_size\n",
    ")\n",
    "\n",
    "print(f\"Chosen shape: {chosen_ids.shape}\")\n",
    "print(f\"Rejected shape: {rejected_ids.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: DPO Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def train_dpo(policy_model, ref_model, chosen_ids, rejected_ids, \n              chosen_labels, rejected_labels, epochs=500, lr=1e-5, beta=0.1):\n    \"\"\"\n    Train with DPO.\n    \"\"\"\n    policy_model.train()\n    optimizer = torch.optim.AdamW(policy_model.parameters(), lr=lr)\n    \n    # Move to device\n    chosen_ids = chosen_ids.to(device)\n    rejected_ids = rejected_ids.to(device)\n    chosen_labels = chosen_labels.to(device)\n    rejected_labels = rejected_labels.to(device)\n    \n    losses = []\n    metrics_history = []\n    \n    for epoch in range(epochs):\n        loss, metrics = dpo_loss(\n            policy_model, ref_model,\n            chosen_ids, rejected_ids,\n            chosen_labels, rejected_labels,\n            beta=beta\n        )\n        \n        optimizer.zero_grad()\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(policy_model.parameters(), 1.0)\n        optimizer.step()\n        \n        losses.append(loss.item())\n        metrics_history.append(metrics)\n        \n        if epoch % 25 == 0:\n            print(f\"Epoch {epoch}: Loss={loss.item():.4f}, \"\n                  f\"Acc={metrics['accuracy']:.2%}, \"\n                  f\"Margin={metrics['reward_margin']:.4f}\")\n    \n    return losses, metrics_history\n\n# Educational note: 50 epochs on small model for fast demo\nprint(\"Training with DPO...\")\ndpo_losses, dpo_metrics = train_dpo(\n    policy_model, ref_model,\n    chosen_ids, rejected_ids,\n    chosen_labels, rejected_labels,\n    epochs=50, lr=1e-5, beta=0.1\n)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training metrics\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "axes[0].plot(dpo_losses)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('DPO Loss')\n",
    "axes[0].set_title('DPO Training Loss')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot([m['accuracy'] for m in dpo_metrics])\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_title('Preference Accuracy')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "axes[2].plot([m['chosen_reward'] for m in dpo_metrics], label='Chosen')\n",
    "axes[2].plot([m['rejected_reward'] for m in dpo_metrics], label='Rejected')\n",
    "axes[2].set_xlabel('Epoch')\n",
    "axes[2].set_ylabel('Implicit Reward')\n",
    "axes[2].set_title('Reward Separation')\n",
    "axes[2].legend()\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Test the Aligned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate(model, instruction, max_tokens=100, temperature=0.7):\n",
    "    \"\"\"Generate a response.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    prompt = f\"\"\"### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "    \n",
    "    tokens = [stoi.get(ch, 0) for ch in prompt]\n",
    "    generated = list(prompt)\n",
    "    \n",
    "    for _ in range(max_tokens):\n",
    "        context = tokens[-block_size:] if len(tokens) >= block_size else tokens\n",
    "        x = torch.tensor([context]).to(device)\n",
    "        \n",
    "        logits = model(x)[0, -1, :] / temperature\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        next_idx = torch.multinomial(probs, 1).item()\n",
    "        \n",
    "        tokens.append(next_idx)\n",
    "        generated.append(itos[next_idx])\n",
    "        \n",
    "        if '<|endoftext|>' in ''.join(generated[-15:]):\n",
    "            break\n",
    "    \n",
    "    response = ''.join(generated)\n",
    "    if \"### Response:\" in response:\n",
    "        response = response.split(\"### Response:\")[-1].strip()\n",
    "        response = response.replace(\"<|endoftext|>\", \"\").strip()\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on training examples\n",
    "print(\"=\" * 60)\n",
    "print(\"TESTING ALIGNED MODEL\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "test_prompts = [\n",
    "    \"What is the capital of France?\",\n",
    "    \"Say hello.\",\n",
    "    \"What is 2 + 2?\",\n",
    "    \"Tell me something mean.\",  # Safety test\n",
    "    \"List three fruits.\",\n",
    "]\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    print(f\"\\nQ: {prompt}\")\n",
    "    print(f\"A: {generate(policy_model, prompt)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with reference model (before DPO)\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"COMPARING: BEFORE vs AFTER DPO\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for prompt in [\"Tell me something mean.\", \"Say something rude.\"]:\n",
    "    print(f\"\\nPrompt: {prompt}\")\n",
    "    print(f\"Reference (before DPO): {generate(ref_model, prompt)}\")\n",
    "    print(f\"Policy (after DPO): {generate(policy_model, prompt)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Save Models\n\nExport both the aligned policy model and reference model for comparison and deployment.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import os\n\n# Create models directory\nos.makedirs('../models', exist_ok=True)\n\n# Save DPO-aligned model\ncheckpoint_policy = {\n    'model_state_dict': policy_model.state_dict(),\n    'model_config': {\n        'vocab_size': vocab_size,\n        'd_model': d_model,\n        'n_heads': n_heads,\n        'n_layers': n_layers,\n        'block_size': block_size,\n    },\n    'stoi': stoi,\n    'itos': itos,\n    'dpo_metrics': dpo_metrics[-1] if dpo_metrics else None,\n}\ntorch.save(checkpoint_policy, '../models/dpo_aligned.pt')\nprint(f\"DPO-aligned model saved to ../models/dpo_aligned.pt\")\n\n# Save reference model (before DPO) for comparison\ncheckpoint_ref = {\n    'model_state_dict': ref_model.state_dict(),\n    'model_config': {\n        'vocab_size': vocab_size,\n        'd_model': d_model,\n        'n_heads': n_heads,\n        'n_layers': n_layers,\n        'block_size': block_size,\n    },\n    'stoi': stoi,\n    'itos': itos,\n}\ntorch.save(checkpoint_ref, '../models/sft_reference.pt')\nprint(f\"Reference model saved to ../models/sft_reference.pt\")\n\n# Summary\nprint(f\"\\nModels saved:\")\nprint(f\"  - dpo_aligned.pt: Policy after DPO training\")\nprint(f\"  - sft_reference.pt: Reference model (SFT only)\")\nprint(f\"  - Final DPO accuracy: {dpo_metrics[-1]['accuracy']:.2%}\" if dpo_metrics else \"\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Clean Up GPU Memory",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import gc\n\n# Delete models and data tensors\ndel policy_model\ndel ref_model\ndel chosen_ids, rejected_ids, chosen_labels, rejected_labels\ndel X_sft, Y_sft\n\n# Clear CUDA cache\nif torch.cuda.is_available():\n    torch.cuda.empty_cache()\n    torch.cuda.synchronize()\n\ngc.collect()\n\nprint(\"GPU memory cleared!\")\nif torch.cuda.is_available():\n    print(f\"GPU memory allocated: {torch.cuda.memory_allocated() / 1024**2:.1f} MB\")\n    print(f\"GPU memory cached: {torch.cuda.memory_reserved() / 1024**2:.1f} MB\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding What DPO Does\n",
    "\n",
    "DPO teaches the model to:\n",
    "\n",
    "1. **Increase probability** of preferred responses\n",
    "2. **Decrease probability** of rejected responses\n",
    "3. **Stay close to reference** to maintain general capabilities\n",
    "\n",
    "The $\\beta$ parameter controls this tradeoff:\n",
    "- High $\\beta$: Stronger preference learning, might deviate more from reference\n",
    "- Low $\\beta$: Weaker preference learning, stays closer to reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Complete LLM Pipeline\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────┐\n",
    "│                  THE COMPLETE LLM PIPELINE                   │\n",
    "└─────────────────────────────────────────────────────────────┘\n",
    "\n",
    "Part 1-2: PRETRAINING\n",
    "├── Character-level language modeling\n",
    "├── Learn language patterns from raw text\n",
    "└── Names dataset → Shakespeare\n",
    "\n",
    "Part 3: TOKENIZATION  \n",
    "├── BPE: Subword tokenization\n",
    "├── Efficient representation\n",
    "└── Handles any text\n",
    "\n",
    "Part 4: ARCHITECTURE\n",
    "├── Self-attention mechanism  \n",
    "├── Transformer blocks\n",
    "└── Positional encoding\n",
    "\n",
    "Part 5: INSTRUCTION TUNING (SFT)\n",
    "├── Teach instruction-following format\n",
    "├── (instruction, response) pairs\n",
    "└── Standard supervised learning\n",
    "\n",
    "Part 6: ALIGNMENT (DPO)\n",
    "├── Teach preferences\n",
    "├── (instruction, chosen, rejected) triplets\n",
    "└── Direct preference optimization\n",
    "\n",
    "                        ↓\n",
    "              ALIGNED LANGUAGE MODEL\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "We implemented DPO from scratch:\n",
    "\n",
    "| Component | Purpose |\n",
    "|-----------|---------|\n",
    "| Preference data | (prompt, chosen, rejected) triplets |\n",
    "| Reference model | Frozen copy, prevents drift |\n",
    "| Log probability | Measure model's \"confidence\" |\n",
    "| DPO loss | Push chosen up, rejected down |\n",
    "| β parameter | Strength of preference learning |\n",
    "\n",
    "**Key insight**: DPO eliminates the need for a separate reward model by implicitly encoding the reward function in the policy itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congratulations!\n",
    "\n",
    "You've built an LLM from scratch through all 6 parts:\n",
    "\n",
    "1. **Character-Level LM**: Basic next-token prediction\n",
    "2. **Shakespeare**: Same model, different domain\n",
    "3. **BPE Tokenizer**: Efficient subword tokenization\n",
    "4. **Self-Attention**: The transformer architecture\n",
    "5. **Instruction Tuning**: Following user instructions\n",
    "6. **DPO Alignment**: Learning human preferences\n",
    "\n",
    "This is the complete pipeline used by modern LLMs like GPT-4 and Claude!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. **Vary β**: Try β=0.01, 0.1, 1.0. How does it affect learning?\n",
    "2. **More preferences**: Add 50 more preference pairs. Does accuracy improve?\n",
    "3. **IPO**: Implement Identity Preference Optimization (simpler variant)\n",
    "4. **KTO**: Implement Kahneman-Tversky Optimization (only needs good/bad labels)\n",
    "5. **Rejection sampling**: Generate multiple responses, rank them, use for DPO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
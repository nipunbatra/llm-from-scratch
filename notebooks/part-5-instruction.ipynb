{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5: Instruction Tuning\n",
    "## From text completion to following instructions\n",
    "\n",
    "In Parts 1-4, we built models that complete text. Give them a prompt, they continue it. But that's not how we use ChatGPT or Claude:\n",
    "\n",
    "**Pretrained model**:\n",
    "```\n",
    "Input: \"What is the capital of France?\"\n",
    "Output: \"What is the capital of Germany? What is the capital of Spain?...\"\n",
    "```\n",
    "\n",
    "**Instruction-tuned model**:\n",
    "```\n",
    "Input: \"What is the capital of France?\"\n",
    "Output: \"Paris\"\n",
    "```\n",
    "\n",
    "The pretrained model doesn't know it should **answer** the question—it just continues the text pattern."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Instruction Tuning?\n",
    "\n",
    "Instruction tuning (also called supervised fine-tuning or SFT) teaches the model to:\n",
    "\n",
    "1. Recognize that an input is a question/instruction\n",
    "2. Generate an appropriate response\n",
    "3. Stop after answering\n",
    "\n",
    "We do this by fine-tuning on (instruction, response) pairs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "import math\n",
    "import os\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.manual_seed(42)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Create an Instruction Dataset\n",
    "\n",
    "For a real model, we'd use datasets like Alpaca, Dolly, or OpenAssistant. For our educational purposes, we'll create a small synthetic dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our mini instruction dataset\n",
    "instruction_data = [\n",
    "    # Simple Q&A\n",
    "    {\"instruction\": \"What is the capital of France?\", \"response\": \"The capital of France is Paris.\"},\n",
    "    {\"instruction\": \"What is the capital of India?\", \"response\": \"The capital of India is New Delhi.\"},\n",
    "    {\"instruction\": \"What is the capital of Japan?\", \"response\": \"The capital of Japan is Tokyo.\"},\n",
    "    {\"instruction\": \"What is the capital of Germany?\", \"response\": \"The capital of Germany is Berlin.\"},\n",
    "    {\"instruction\": \"What is the capital of Italy?\", \"response\": \"The capital of Italy is Rome.\"},\n",
    "    \n",
    "    # Math\n",
    "    {\"instruction\": \"What is 2 + 2?\", \"response\": \"2 + 2 equals 4.\"},\n",
    "    {\"instruction\": \"What is 5 * 3?\", \"response\": \"5 * 3 equals 15.\"},\n",
    "    {\"instruction\": \"What is 10 - 7?\", \"response\": \"10 - 7 equals 3.\"},\n",
    "    {\"instruction\": \"What is 20 / 4?\", \"response\": \"20 / 4 equals 5.\"},\n",
    "    \n",
    "    # Definitions\n",
    "    {\"instruction\": \"Define machine learning.\", \"response\": \"Machine learning is a type of artificial intelligence where computers learn patterns from data.\"},\n",
    "    {\"instruction\": \"What is Python?\", \"response\": \"Python is a popular programming language known for its simplicity and readability.\"},\n",
    "    {\"instruction\": \"What is an algorithm?\", \"response\": \"An algorithm is a step-by-step procedure for solving a problem or performing a task.\"},\n",
    "    \n",
    "    # Instructions\n",
    "    {\"instruction\": \"Say hello.\", \"response\": \"Hello!\"},\n",
    "    {\"instruction\": \"Count to five.\", \"response\": \"1, 2, 3, 4, 5.\"},\n",
    "    {\"instruction\": \"Name three colors.\", \"response\": \"Red, blue, and green.\"},\n",
    "    {\"instruction\": \"List three fruits.\", \"response\": \"Apple, banana, and orange.\"},\n",
    "    \n",
    "    # More Q&A\n",
    "    {\"instruction\": \"Who wrote Romeo and Juliet?\", \"response\": \"William Shakespeare wrote Romeo and Juliet.\"},\n",
    "    {\"instruction\": \"What planet is closest to the sun?\", \"response\": \"Mercury is the planet closest to the sun.\"},\n",
    "    {\"instruction\": \"How many days are in a week?\", \"response\": \"There are 7 days in a week.\"},\n",
    "    {\"instruction\": \"What is H2O?\", \"response\": \"H2O is the chemical formula for water.\"},\n",
    "]\n",
    "\n",
    "print(f\"Number of instruction-response pairs: {len(instruction_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Format for Training\n",
    "\n",
    "We need a consistent format that the model can learn. A common format is:\n",
    "\n",
    "```\n",
    "### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Response:\n",
    "{response}<|endoftext|>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_example(example):\n",
    "    \"\"\"Format an instruction-response pair for training.\"\"\"\n",
    "    return f\"\"\"### Instruction:\n",
    "{example['instruction']}\n",
    "\n",
    "### Response:\n",
    "{example['response']}<|endoftext|>\"\"\"\n",
    "\n",
    "# Preview formatting\n",
    "print(\"Example formatted training data:\")\n",
    "print(\"=\" * 50)\n",
    "print(format_example(instruction_data[0]))\n",
    "print(\"=\" * 50)\n",
    "print(format_example(instruction_data[5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training text\n",
    "training_text = \"\\n\\n\".join(format_example(ex) for ex in instruction_data)\n",
    "print(f\"Total training characters: {len(training_text)}\")\n",
    "print(f\"\\nFirst 500 characters:\")\n",
    "print(training_text[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Build Vocabulary and Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build vocabulary from training data\n",
    "chars = sorted(set(training_text))\n",
    "stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "itos = {i: ch for ch, i in stoi.items()}\n",
    "vocab_size = len(stoi)\n",
    "\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "print(f\"Characters: {''.join(chars[:50])}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode training data\n",
    "block_size = 128\n",
    "\n",
    "def build_dataset(text, block_size, stoi):\n",
    "    data = [stoi[ch] for ch in text]\n",
    "    X, Y = [], []\n",
    "    for i in range(len(data) - block_size):\n",
    "        X.append(data[i:i + block_size])\n",
    "        Y.append(data[i + 1:i + block_size + 1])\n",
    "    return torch.tensor(X), torch.tensor(Y)\n",
    "\n",
    "X, Y = build_dataset(training_text, block_size, stoi)\n",
    "print(f\"Training examples: {len(X)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Model Architecture\n",
    "\n",
    "We'll use the transformer from Part 4, but with a twist: we'll initialize with pretrained weights in a real scenario. For simplicity, we'll train from scratch on our small dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.shape[1]]\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_heads):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = d_model // n_heads\n",
    "        self.W_qkv = nn.Linear(d_model, 3 * d_model, bias=False)\n",
    "        self.W_out = nn.Linear(d_model, d_model, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch, seq_len, d_model = x.shape\n",
    "        qkv = self.W_qkv(x).reshape(batch, seq_len, 3, self.n_heads, self.d_k)\n",
    "        qkv = qkv.permute(2, 0, 3, 1, 4)\n",
    "        Q, K, V = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        mask = torch.triu(torch.ones(seq_len, seq_len, device=x.device), diagonal=1).bool()\n",
    "        scores = scores.masked_fill(mask, float('-inf'))\n",
    "\n",
    "        attention = F.softmax(scores, dim=-1)\n",
    "        output = torch.matmul(attention, V)\n",
    "        output = output.permute(0, 2, 1, 3).reshape(batch, seq_len, d_model)\n",
    "        return self.W_out(output)\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadAttention(d_model, n_heads)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, 4 * d_model),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * d_model, d_model),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.dropout(self.attention(self.ln1(x)))\n",
    "        x = x + self.ffn(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class InstructionLM(nn.Module):\n",
    "    \"\"\"Transformer for instruction following.\"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, d_model, n_heads, n_layers, block_size, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.token_emb = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_enc = PositionalEncoding(d_model, block_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(d_model, n_heads, dropout)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "        self.ln_final = nn.LayerNorm(d_model)\n",
    "        self.output = nn.Linear(d_model, vocab_size)\n",
    "        self.block_size = block_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.token_emb(x)\n",
    "        x = self.pos_enc(x)\n",
    "        x = self.dropout(x)\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        x = self.ln_final(x)\n",
    "        return self.output(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "d_model = 128\n",
    "n_heads = 4\n",
    "n_layers = 4\n",
    "\n",
    "model = InstructionLM(\n",
    "    vocab_size=vocab_size,\n",
    "    d_model=d_model,\n",
    "    n_heads=n_heads,\n",
    "    n_layers=n_layers,\n",
    "    block_size=block_size,\n",
    "    dropout=0.1\n",
    ").to(device)\n",
    "\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Model parameters: {num_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, X, Y, epochs=1000, batch_size=32, lr=3e-4):\n",
    "    model.train()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "    X, Y = X.to(device), Y.to(device)\n",
    "    losses = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        perm = torch.randperm(X.shape[0])\n",
    "        total_loss, n_batches = 0, 0\n",
    "\n",
    "        for i in range(0, len(X), batch_size):\n",
    "            idx = perm[i:i+batch_size]\n",
    "            x_batch, y_batch = X[idx], Y[idx]\n",
    "\n",
    "            logits = model(x_batch)\n",
    "            loss = F.cross_entropy(logits.view(-1, vocab_size), y_batch.view(-1))\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            n_batches += 1\n",
    "\n",
    "        losses.append(total_loss / n_batches)\n",
    "        if epoch % 200 == 0:\n",
    "            print(f\"Epoch {epoch}: Loss = {losses[-1]:.4f}\")\n",
    "\n",
    "    return losses\n",
    "\n",
    "losses = train(model, X, Y, epochs=2000, batch_size=32, lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Instruction Tuning Loss')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Inference with Instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def ask(model, instruction, max_tokens=100, temperature=0.7):\n",
    "    \"\"\"\n",
    "    Ask the model a question in instruction format.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    # Format the prompt\n",
    "    prompt = f\"\"\"### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "\n",
    "    # Encode prompt\n",
    "    tokens = [stoi.get(ch, 0) for ch in prompt]\n",
    "    generated = list(prompt)\n",
    "\n",
    "    for _ in range(max_tokens):\n",
    "        # Use last block_size tokens\n",
    "        context = tokens[-block_size:] if len(tokens) >= block_size else tokens\n",
    "        x = torch.tensor([context]).to(device)\n",
    "\n",
    "        logits = model(x)\n",
    "        logits = logits[0, -1, :] / temperature\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        next_idx = torch.multinomial(probs, 1).item()\n",
    "\n",
    "        next_char = itos[next_idx]\n",
    "        tokens.append(next_idx)\n",
    "        generated.append(next_char)\n",
    "\n",
    "        # Stop at end-of-text token or double newline\n",
    "        if '<|endoftext|>' in ''.join(generated[-15:]):\n",
    "            break\n",
    "\n",
    "    response = ''.join(generated)\n",
    "\n",
    "    # Extract just the response part\n",
    "    if \"### Response:\" in response:\n",
    "        response = response.split(\"### Response:\")[-1].strip()\n",
    "        response = response.replace(\"<|endoftext|>\", \"\").strip()\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with training examples\n",
    "print(\"=\" * 60)\n",
    "print(\"TESTING ON TRAINING EXAMPLES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "test_questions = [\n",
    "    \"What is the capital of France?\",\n",
    "    \"What is 2 + 2?\",\n",
    "    \"Who wrote Romeo and Juliet?\",\n",
    "    \"Say hello.\",\n",
    "]\n",
    "\n",
    "for q in test_questions:\n",
    "    print(f\"\\nQ: {q}\")\n",
    "    print(f\"A: {ask(model, q)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test generalization (these weren't in training!)\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TESTING GENERALIZATION (not in training)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "novel_questions = [\n",
    "    \"What is the capital of Spain?\",  # Similar pattern, different answer\n",
    "    \"What is 3 + 3?\",                  # Similar pattern\n",
    "    \"Name three animals.\",             # Similar to \"Name three colors\"\n",
    "    \"Count to three.\",                 # Similar to \"Count to five\"\n",
    "]\n",
    "\n",
    "for q in novel_questions:\n",
    "    print(f\"\\nQ: {q}\")\n",
    "    print(f\"A: {ask(model, q)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generalization Limits\n",
    "\n",
    "With only 20 training examples, generalization is limited. Real instruction-tuned models like Alpaca use 52K examples, and models like GPT-3.5 use millions.\n",
    "\n",
    "However, even with few examples, the model learns the **format**: instruction → response pattern."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Understanding What Changed\n",
    "\n",
    "The key insight: **instruction tuning doesn't add new knowledge**—it teaches the model to **access and format** its knowledge appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see what happens WITHOUT instruction format\n",
    "@torch.no_grad()\n",
    "def complete(model, prompt, max_tokens=100, temperature=0.7):\n",
    "    \"\"\"Raw text completion without instruction formatting.\"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    tokens = [stoi.get(ch, 0) for ch in prompt]\n",
    "    generated = list(prompt)\n",
    "\n",
    "    for _ in range(max_tokens):\n",
    "        context = tokens[-block_size:]\n",
    "        x = torch.tensor([context]).to(device)\n",
    "\n",
    "        logits = model(x)[0, -1, :] / temperature\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        next_idx = torch.multinomial(probs, 1).item()\n",
    "\n",
    "        tokens.append(next_idx)\n",
    "        generated.append(itos[next_idx])\n",
    "\n",
    "    return ''.join(generated)\n",
    "\n",
    "print(\"Raw completion (no instruction format):\")\n",
    "print(\"-\" * 50)\n",
    "print(complete(model, \"What is the capital of France?\", max_tokens=80))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With instruction format, the model knows to:\n",
    "1. Look for \"### Instruction:\" marker\n",
    "2. Generate a direct response after \"### Response:\"\n",
    "3. Stop at the end-of-text token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Real-World Instruction Tuning Pipeline\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────┐\n",
    "│                 INSTRUCTION TUNING PIPELINE                  │\n",
    "└─────────────────────────────────────────────────────────────┘\n",
    "\n",
    "1. PRETRAIN on massive text corpus\n",
    "   └── Model learns language patterns, facts, reasoning\n",
    "\n",
    "2. COLLECT instruction data\n",
    "   ├── Human-written (expensive, high quality)\n",
    "   ├── GPT-generated (Alpaca approach)\n",
    "   └── From existing NLP datasets\n",
    "\n",
    "3. FORMAT data consistently\n",
    "   └── Instruction: {task}\n",
    "       Response: {answer}\n",
    "\n",
    "4. FINE-TUNE on instruction data\n",
    "   ├── Usually 1-3 epochs\n",
    "   ├── Lower learning rate than pretraining\n",
    "   └── Careful not to forget pretrained knowledge\n",
    "\n",
    "5. EVALUATE\n",
    "   ├── Held-out instructions\n",
    "   ├── Human evaluation\n",
    "   └── Benchmark tasks\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "We implemented instruction tuning from scratch:\n",
    "\n",
    "| Aspect | What We Did |\n",
    "|--------|-------------|\n",
    "| Dataset | 20 instruction-response pairs |\n",
    "| Format | \"### Instruction:\\n{q}\\n\\n### Response:\\n{a}\" |\n",
    "| Model | Same transformer from Part 4 |\n",
    "| Training | Standard next-token prediction |\n",
    "| Result | Model follows instruction format |\n",
    "\n",
    "**Key insight**: Instruction tuning is just supervised fine-tuning on carefully formatted data. The magic is in the data, not the training procedure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's Next\n",
    "\n",
    "In **Part 6**, we'll add **DPO (Direct Preference Optimization)**—training the model to prefer good responses over bad ones. This is the final step toward alignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. **More data**: Add 50 more instruction-response pairs. How does quality improve?\n",
    "2. **Different formats**: Try \"User: {q}\\nAssistant: {a}\" format\n",
    "3. **Multi-turn**: Add conversation examples with multiple turns\n",
    "4. **Negative examples**: What if you include bad examples? (Spoiler: Part 6!)\n",
    "5. **LoRA**: Implement low-rank adaptation for efficient fine-tuning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

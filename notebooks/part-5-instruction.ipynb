{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5: Instruction Tuning\n",
    "## From text completion to following instructions\n",
    "\n",
    "In Parts 1-4, we built models that complete text. Give them a prompt, they continue it. But that's not how we use ChatGPT or Claude:\n",
    "\n",
    "**Pretrained model**:\n",
    "```\n",
    "Input: \"What is the capital of France?\"\n",
    "Output: \"What is the capital of Germany? What is the capital of Spain?...\"\n",
    "```\n",
    "\n",
    "**Instruction-tuned model**:\n",
    "```\n",
    "Input: \"What is the capital of France?\"\n",
    "Output: \"Paris\"\n",
    "```\n",
    "\n",
    "The pretrained model doesn't know it should **answer** the question—it just continues the text pattern."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Instruction Tuning?\n",
    "\n",
    "Instruction tuning (also called supervised fine-tuning or SFT) teaches the model to:\n",
    "\n",
    "1. Recognize that an input is a question/instruction\n",
    "2. Generate an appropriate response\n",
    "3. Stop after answering\n",
    "\n",
    "We do this by fine-tuning on (instruction, response) pairs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "import math\n",
    "import os\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.manual_seed(42)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Create an Instruction Dataset\n",
    "\n",
    "For a real model, we'd use datasets like Alpaca, Dolly, or OpenAssistant. For our educational purposes, we'll create a small synthetic dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of instruction-response pairs: 77\n",
      "Categories covered: General Q&A, Math, Python basics, Data types, Algorithms, ML\n"
     ]
    }
   ],
   "source": [
    "# Our expanded instruction dataset with Python Q&A\n",
    "instruction_data = [\n",
    "    # === GENERAL Q&A ===\n",
    "    {\"instruction\": \"What is the capital of France?\", \"response\": \"The capital of France is Paris.\"},\n",
    "    {\"instruction\": \"What is the capital of India?\", \"response\": \"The capital of India is New Delhi.\"},\n",
    "    {\"instruction\": \"What is the capital of Japan?\", \"response\": \"The capital of Japan is Tokyo.\"},\n",
    "    {\"instruction\": \"What is the capital of Germany?\", \"response\": \"The capital of Germany is Berlin.\"},\n",
    "    {\"instruction\": \"What is the capital of Italy?\", \"response\": \"The capital of Italy is Rome.\"},\n",
    "    {\"instruction\": \"What is the capital of Spain?\", \"response\": \"The capital of Spain is Madrid.\"},\n",
    "    {\"instruction\": \"What is the capital of China?\", \"response\": \"The capital of China is Beijing.\"},\n",
    "    {\"instruction\": \"What is the capital of Brazil?\", \"response\": \"The capital of Brazil is Brasilia.\"},\n",
    "    \n",
    "    # Math\n",
    "    {\"instruction\": \"What is 2 + 2?\", \"response\": \"2 + 2 equals 4.\"},\n",
    "    {\"instruction\": \"What is 5 * 3?\", \"response\": \"5 * 3 equals 15.\"},\n",
    "    {\"instruction\": \"What is 10 - 7?\", \"response\": \"10 - 7 equals 3.\"},\n",
    "    {\"instruction\": \"What is 20 / 4?\", \"response\": \"20 / 4 equals 5.\"},\n",
    "    {\"instruction\": \"What is 3 + 3?\", \"response\": \"3 + 3 equals 6.\"},\n",
    "    {\"instruction\": \"What is 7 * 8?\", \"response\": \"7 * 8 equals 56.\"},\n",
    "    {\"instruction\": \"What is 100 / 10?\", \"response\": \"100 / 10 equals 10.\"},\n",
    "    \n",
    "    # Basic instructions\n",
    "    {\"instruction\": \"Say hello.\", \"response\": \"Hello! How can I help you today?\"},\n",
    "    {\"instruction\": \"Count to five.\", \"response\": \"1, 2, 3, 4, 5.\"},\n",
    "    {\"instruction\": \"Count to three.\", \"response\": \"1, 2, 3.\"},\n",
    "    {\"instruction\": \"Name three colors.\", \"response\": \"Red, blue, and green.\"},\n",
    "    {\"instruction\": \"List three fruits.\", \"response\": \"Apple, banana, and orange.\"},\n",
    "    {\"instruction\": \"Name three animals.\", \"response\": \"Dog, cat, and elephant.\"},\n",
    "    {\"instruction\": \"List three countries.\", \"response\": \"USA, India, and Japan.\"},\n",
    "    \n",
    "    # General knowledge\n",
    "    {\"instruction\": \"Who wrote Romeo and Juliet?\", \"response\": \"William Shakespeare wrote Romeo and Juliet.\"},\n",
    "    {\"instruction\": \"What planet is closest to the sun?\", \"response\": \"Mercury is the planet closest to the sun.\"},\n",
    "    {\"instruction\": \"How many days are in a week?\", \"response\": \"There are 7 days in a week.\"},\n",
    "    {\"instruction\": \"What is H2O?\", \"response\": \"H2O is the chemical formula for water.\"},\n",
    "    {\"instruction\": \"How many hours in a day?\", \"response\": \"There are 24 hours in a day.\"},\n",
    "    {\"instruction\": \"How many months in a year?\", \"response\": \"There are 12 months in a year.\"},\n",
    "    \n",
    "    # === PYTHON BASICS ===\n",
    "    {\"instruction\": \"What is Python?\", \"response\": \"Python is a popular programming language known for its simplicity and readability.\"},\n",
    "    {\"instruction\": \"How do I print Hello World in Python?\", \"response\": \"Use print('Hello World') to print Hello World in Python.\"},\n",
    "    {\"instruction\": \"What is a variable in Python?\", \"response\": \"A variable is a name that stores a value. Example: x = 5 stores 5 in x.\"},\n",
    "    {\"instruction\": \"How do I create a list in Python?\", \"response\": \"Create a list with square brackets: my_list = [1, 2, 3]\"},\n",
    "    {\"instruction\": \"What is a function in Python?\", \"response\": \"A function is reusable code defined with def. Example: def greet(): print('Hi')\"},\n",
    "    {\"instruction\": \"How do I define a function in Python?\", \"response\": \"Use def keyword: def my_function(): followed by indented code.\"},\n",
    "    {\"instruction\": \"What is a loop in Python?\", \"response\": \"A loop repeats code. Use for to iterate: for i in range(5): print(i)\"},\n",
    "    {\"instruction\": \"How do I use a for loop?\", \"response\": \"for item in collection: then indented code. Example: for i in range(3): print(i)\"},\n",
    "    {\"instruction\": \"What is an if statement?\", \"response\": \"An if statement runs code conditionally: if x > 5: print('big')\"},\n",
    "    {\"instruction\": \"How do I check if a number is even?\", \"response\": \"Use modulo: if num % 2 == 0: the number is even.\"},\n",
    "    {\"instruction\": \"What is a string in Python?\", \"response\": \"A string is text in quotes: name = 'Python' or name = \\\"Python\\\"\"},\n",
    "    {\"instruction\": \"How do I get the length of a list?\", \"response\": \"Use len(): length = len(my_list) returns the number of items.\"},\n",
    "    {\"instruction\": \"What is a dictionary in Python?\", \"response\": \"A dictionary stores key-value pairs: d = {'name': 'John', 'age': 25}\"},\n",
    "    {\"instruction\": \"How do I add to a list?\", \"response\": \"Use append(): my_list.append(4) adds 4 to the end of my_list.\"},\n",
    "    {\"instruction\": \"What is range() in Python?\", \"response\": \"range(n) generates numbers 0 to n-1. range(5) gives 0,1,2,3,4.\"},\n",
    "    {\"instruction\": \"How do I read a file in Python?\", \"response\": \"Use open(): with open('file.txt', 'r') as f: content = f.read()\"},\n",
    "    {\"instruction\": \"What is a class in Python?\", \"response\": \"A class is a blueprint for objects: class Dog: def bark(self): print('Woof')\"},\n",
    "    {\"instruction\": \"How do I handle errors in Python?\", \"response\": \"Use try/except: try: risky_code() except: handle_error()\"},\n",
    "    {\"instruction\": \"What is None in Python?\", \"response\": \"None represents no value or null. Example: x = None\"},\n",
    "    {\"instruction\": \"How do I convert string to int?\", \"response\": \"Use int(): num = int('42') converts string '42' to integer 42.\"},\n",
    "    {\"instruction\": \"What is a tuple in Python?\", \"response\": \"A tuple is immutable list: t = (1, 2, 3) cannot be changed after creation.\"},\n",
    "    {\"instruction\": \"How do I join strings?\", \"response\": \"Use + or join: 'Hello' + ' World' or ' '.join(['Hello', 'World'])\"},\n",
    "    {\"instruction\": \"What is import in Python?\", \"response\": \"import loads a module: import math then use math.sqrt(16) for square root.\"},\n",
    "    {\"instruction\": \"How do I create a set?\", \"response\": \"Use curly braces: s = {1, 2, 3} creates a set with unique values.\"},\n",
    "    {\"instruction\": \"What is self in Python?\", \"response\": \"self refers to the current object instance in a class method.\"},\n",
    "    {\"instruction\": \"How do I reverse a list?\", \"response\": \"Use reverse() method: my_list.reverse() or my_list[::-1] for new list.\"},\n",
    "    {\"instruction\": \"What is pip?\", \"response\": \"pip is Python's package manager. Use pip install package_name to install.\"},\n",
    "    {\"instruction\": \"How do I check Python version?\", \"response\": \"Run python --version in terminal or import sys; print(sys.version)\"},\n",
    "    {\"instruction\": \"What is a boolean?\", \"response\": \"Boolean is True or False. Used in conditions: if is_valid: do_something()\"},\n",
    "    {\"instruction\": \"How do I sort a list?\", \"response\": \"Use sort(): my_list.sort() or sorted(my_list) for new sorted list.\"},\n",
    "    \n",
    "    # === PYTHON DATA TYPES ===\n",
    "    {\"instruction\": \"What data types exist in Python?\", \"response\": \"Python has int, float, str, list, dict, tuple, set, bool, and None.\"},\n",
    "    {\"instruction\": \"What is the difference between list and tuple?\", \"response\": \"Lists are mutable (changeable), tuples are immutable (fixed after creation).\"},\n",
    "    {\"instruction\": \"How do I check type of a variable?\", \"response\": \"Use type(): type(x) returns the type, like <class 'int'> or <class 'str'>\"},\n",
    "    {\"instruction\": \"What is a float in Python?\", \"response\": \"A float is a decimal number: x = 3.14 or y = 2.0\"},\n",
    "    \n",
    "    # === ALGORITHMS ===\n",
    "    {\"instruction\": \"What is an algorithm?\", \"response\": \"An algorithm is a step-by-step procedure for solving a problem or task.\"},\n",
    "    {\"instruction\": \"What is Big O notation?\", \"response\": \"Big O describes algorithm efficiency. O(1) is constant, O(n) is linear time.\"},\n",
    "    {\"instruction\": \"What is a sorting algorithm?\", \"response\": \"A sorting algorithm arranges items in order. Examples: bubble sort, quicksort.\"},\n",
    "    {\"instruction\": \"What is recursion?\", \"response\": \"Recursion is when a function calls itself. Needs a base case to stop.\"},\n",
    "    {\"instruction\": \"What is binary search?\", \"response\": \"Binary search finds items in sorted list by halving. O(log n) time.\"},\n",
    "    \n",
    "    # === MACHINE LEARNING ===\n",
    "    {\"instruction\": \"What is machine learning?\", \"response\": \"Machine learning is AI where computers learn patterns from data to make predictions.\"},\n",
    "    {\"instruction\": \"What is deep learning?\", \"response\": \"Deep learning uses neural networks with many layers to learn complex patterns.\"},\n",
    "    {\"instruction\": \"What is a neural network?\", \"response\": \"A neural network is layers of connected nodes that learn from data.\"},\n",
    "    {\"instruction\": \"What is training in ML?\", \"response\": \"Training is teaching a model by showing it examples and adjusting weights.\"},\n",
    "    {\"instruction\": \"What is overfitting?\", \"response\": \"Overfitting is when a model memorizes training data but fails on new data.\"},\n",
    "    {\"instruction\": \"What is a loss function?\", \"response\": \"A loss function measures how wrong predictions are. Training minimizes it.\"},\n",
    "    {\"instruction\": \"What is PyTorch?\", \"response\": \"PyTorch is a Python library for deep learning with dynamic computation graphs.\"},\n",
    "    {\"instruction\": \"What is a tensor?\", \"response\": \"A tensor is a multi-dimensional array, like a matrix but with more dimensions.\"},\n",
    "    {\"instruction\": \"What is backpropagation?\", \"response\": \"Backpropagation computes gradients to update weights during training.\"},\n",
    "    {\"instruction\": \"What is a learning rate?\", \"response\": \"Learning rate controls how much to adjust weights. Too high = unstable, too low = slow.\"},\n",
    "]\n",
    "\n",
    "print(f\"Number of instruction-response pairs: {len(instruction_data)}\")\n",
    "print(f\"Categories covered: General Q&A, Math, Python basics, Data types, Algorithms, ML\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Format for Training\n",
    "\n",
    "We need a consistent format that the model can learn. A common format is:\n",
    "\n",
    "```\n",
    "### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Response:\n",
    "{response}<|endoftext|>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example formatted training data:\n",
      "==================================================\n",
      "### Instruction:\n",
      "What is the capital of France?\n",
      "\n",
      "### Response:\n",
      "The capital of France is Paris.<|endoftext|>\n",
      "==================================================\n",
      "### Instruction:\n",
      "What is the capital of Spain?\n",
      "\n",
      "### Response:\n",
      "The capital of Spain is Madrid.<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "def format_example(example):\n",
    "    \"\"\"Format an instruction-response pair for training.\"\"\"\n",
    "    return f\"\"\"### Instruction:\n",
    "{example['instruction']}\n",
    "\n",
    "### Response:\n",
    "{example['response']}<|endoftext|>\"\"\"\n",
    "\n",
    "# Preview formatting\n",
    "print(\"Example formatted training data:\")\n",
    "print(\"=\" * 50)\n",
    "print(format_example(instruction_data[0]))\n",
    "print(\"=\" * 50)\n",
    "print(format_example(instruction_data[5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training characters: 9646\n",
      "\n",
      "First 500 characters:\n",
      "### Instruction:\n",
      "What is the capital of France?\n",
      "\n",
      "### Response:\n",
      "The capital of France is Paris.<|endoftext|>\n",
      "\n",
      "### Instruction:\n",
      "What is the capital of India?\n",
      "\n",
      "### Response:\n",
      "The capital of India is New Delhi.<|endoftext|>\n",
      "\n",
      "### Instruction:\n",
      "What is the capital of Japan?\n",
      "\n",
      "### Response:\n",
      "The capital of Japan is Tokyo.<|endoftext|>\n",
      "\n",
      "### Instruction:\n",
      "What is the capital of Germany?\n",
      "\n",
      "### Response:\n",
      "The capital of Germany is Berlin.<|endoftext|>\n",
      "\n",
      "### Instruction:\n",
      "What is the capital of Italy?\n",
      "\n",
      "### Response:\n"
     ]
    }
   ],
   "source": [
    "# Create training text\n",
    "training_text = \"\\n\\n\".join(format_example(ex) for ex in instruction_data)\n",
    "print(f\"Total training characters: {len(training_text)}\")\n",
    "print(f\"\\nFirst 500 characters:\")\n",
    "print(training_text[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Build Vocabulary and Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 82\n",
      "Characters: \n",
      " !\"#%'()*+,-./012345678:;<=>?ABCDEFGHIJLMNOPRSTUW...\n"
     ]
    }
   ],
   "source": [
    "# Build vocabulary from training data\n",
    "chars = sorted(set(training_text))\n",
    "stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "itos = {i: ch for ch, i in stoi.items()}\n",
    "vocab_size = len(stoi)\n",
    "\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "print(f\"Characters: {''.join(chars[:50])}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training examples: 9582 (small block size for fast demo)\n"
     ]
    }
   ],
   "source": [
    "# Encode training data with SMALLER block size\n",
    "block_size = 64  # Reduced from 128 for speed\n",
    "\n",
    "def build_dataset(text, block_size, stoi):\n",
    "    data = [stoi[ch] for ch in text]\n",
    "    X, Y = [], []\n",
    "    for i in range(len(data) - block_size):\n",
    "        X.append(data[i:i + block_size])\n",
    "        Y.append(data[i + 1:i + block_size + 1])\n",
    "    return torch.tensor(X), torch.tensor(Y)\n",
    "\n",
    "X, Y = build_dataset(training_text, block_size, stoi)\n",
    "print(f\"Training examples: {len(X)} (small block size for fast demo)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Model Architecture\n",
    "\n",
    "We'll use the transformer from Part 4, but with a twist: we'll initialize with pretrained weights in a real scenario. For simplicity, we'll train from scratch on our small dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.shape[1]]\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_heads):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = d_model // n_heads\n",
    "        self.W_qkv = nn.Linear(d_model, 3 * d_model, bias=False)\n",
    "        self.W_out = nn.Linear(d_model, d_model, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch, seq_len, d_model = x.shape\n",
    "        qkv = self.W_qkv(x).reshape(batch, seq_len, 3, self.n_heads, self.d_k)\n",
    "        qkv = qkv.permute(2, 0, 3, 1, 4)\n",
    "        Q, K, V = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        mask = torch.triu(torch.ones(seq_len, seq_len, device=x.device), diagonal=1).bool()\n",
    "        scores = scores.masked_fill(mask, float('-inf'))\n",
    "\n",
    "        attention = F.softmax(scores, dim=-1)\n",
    "        output = torch.matmul(attention, V)\n",
    "        output = output.permute(0, 2, 1, 3).reshape(batch, seq_len, d_model)\n",
    "        return self.W_out(output)\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadAttention(d_model, n_heads)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, 4 * d_model),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * d_model, d_model),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.dropout(self.attention(self.ln1(x)))\n",
    "        x = x + self.ffn(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class InstructionLM(nn.Module):\n",
    "    \"\"\"Transformer for instruction following.\"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, d_model, n_heads, n_layers, block_size, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.token_emb = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_enc = PositionalEncoding(d_model, block_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(d_model, n_heads, dropout)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "        self.ln_final = nn.LayerNorm(d_model)\n",
    "        self.output = nn.Linear(d_model, vocab_size)\n",
    "        self.block_size = block_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.token_emb(x)\n",
    "        x = self.pos_enc(x)\n",
    "        x = self.dropout(x)\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        x = self.ln_final(x)\n",
    "        return self.output(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 614,610\n",
      "\n",
      "--- Model Scale Context ---\n",
      "Our model:     ~0.6M parameters (SMALL for fast demo!)\n",
      "GPT-2 Small:   124M parameters  (202x larger)\n",
      "LLaMA-7B:      7B parameters    (11389x larger)\n",
      "Claude/GPT-4:  ~1T+ parameters  (1627048x larger)\n"
     ]
    }
   ],
   "source": [
    "# Create model with REDUCED capacity for fast demo\n",
    "d_model = 128     # Reduced from 256\n",
    "n_heads = 4       # Reduced from 8\n",
    "n_layers = 3      # Reduced from 6\n",
    "\n",
    "model = InstructionLM(\n",
    "    vocab_size=vocab_size,\n",
    "    d_model=d_model,\n",
    "    n_heads=n_heads,\n",
    "    n_layers=n_layers,\n",
    "    block_size=block_size,\n",
    "    dropout=0.1\n",
    ").to(device)\n",
    "\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Model parameters: {num_params:,}\")\n",
    "\n",
    "# Model scale context\n",
    "print(f\"\\n--- Model Scale Context ---\")\n",
    "print(f\"Our model:     ~{num_params/1_000_000:.1f}M parameters (SMALL for fast demo!)\")\n",
    "print(f\"GPT-2 Small:   124M parameters  ({124_000_000/num_params:.0f}x larger)\")\n",
    "print(f\"LLaMA-7B:      7B parameters    ({7_000_000_000/num_params:.0f}x larger)\")\n",
    "print(f\"Claude/GPT-4:  ~1T+ parameters  ({1_000_000_000_000/num_params:.0f}x larger)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training is resumable - you can interrupt and restart!\n",
      "Epoch 0: Loss = 1.8120\n"
     ]
    }
   ],
   "source": [
    "def train(model, X, Y, epochs=1000, batch_size=32, lr=3e-4, checkpoint_path='../models/checkpoint_part5.pt', resume=True):\n",
    "    \"\"\"\n",
    "    Resumable training with checkpoint saving.\n",
    "    \n",
    "    Args:\n",
    "        resume: If True, attempts to resume from checkpoint\n",
    "        checkpoint_path: Path to save/load checkpoints\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "    X, Y = X.to(device), Y.to(device)\n",
    "    losses = []\n",
    "    start_epoch = 0\n",
    "\n",
    "    # Try to resume from checkpoint\n",
    "    if resume and os.path.exists(checkpoint_path):\n",
    "        print(f\"Resuming from checkpoint: {checkpoint_path}\")\n",
    "        checkpoint = torch.load(checkpoint_path, weights_only=False)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        start_epoch = checkpoint['epoch'] + 1\n",
    "        losses = checkpoint['losses']\n",
    "        print(f\"Resumed from epoch {start_epoch}, previous loss: {losses[-1]:.4f}\")\n",
    "\n",
    "    for epoch in range(start_epoch, epochs):\n",
    "        perm = torch.randperm(X.shape[0])\n",
    "        total_loss, n_batches = 0, 0\n",
    "\n",
    "        for i in range(0, len(X), batch_size):\n",
    "            idx = perm[i:i+batch_size]\n",
    "            x_batch, y_batch = X[idx], Y[idx]\n",
    "\n",
    "            logits = model(x_batch)\n",
    "            loss = F.cross_entropy(logits.view(-1, vocab_size), y_batch.view(-1))\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            n_batches += 1\n",
    "\n",
    "        losses.append(total_loss / n_batches)\n",
    "        \n",
    "        if epoch % 50 == 0:\n",
    "            print(f\"Epoch {epoch}: Loss = {losses[-1]:.4f}\")\n",
    "        \n",
    "        # Save checkpoint every 25 epochs\n",
    "        if (epoch + 1) % 25 == 0:\n",
    "            os.makedirs(os.path.dirname(checkpoint_path), exist_ok=True)\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'losses': losses,\n",
    "            }, checkpoint_path)\n",
    "\n",
    "    print(f\"Training complete! Final loss: {losses[-1]:.4f}\")\n",
    "    return losses\n",
    "\n",
    "# Educational note: 100 epochs on small model for fast demo\n",
    "# For production: use larger model and 1000+ epochs\n",
    "# Training is resumable - interrupt and re-run this cell to continue!\n",
    "print(\"Training is resumable - you can interrupt and restart!\")\n",
    "losses = train(model, X, Y, epochs=100, batch_size=64, lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "plt.figure(figsize=(10, 4))\nplt.plot(losses)\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Instruction Tuning Loss')\nplt.grid(True, alpha=0.3)\n# Show number of epochs trained\nplt.text(0.02, 0.98, f'Total epochs: {len(losses)}', \n         transform=plt.gca().transAxes, verticalalignment='top',\n         bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\nplt.show()\n\nprint(f\"Training summary: {len(losses)} epochs completed, final loss: {losses[-1]:.4f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Inference with Instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def ask(model, instruction, max_tokens=100, temperature=0.7):\n",
    "    \"\"\"\n",
    "    Ask the model a question in instruction format.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    # Format the prompt\n",
    "    prompt = f\"\"\"### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "\n",
    "    # Encode prompt\n",
    "    tokens = [stoi.get(ch, 0) for ch in prompt]\n",
    "    generated = list(prompt)\n",
    "\n",
    "    for _ in range(max_tokens):\n",
    "        # Use last block_size tokens\n",
    "        context = tokens[-block_size:] if len(tokens) >= block_size else tokens\n",
    "        x = torch.tensor([context]).to(device)\n",
    "\n",
    "        logits = model(x)\n",
    "        logits = logits[0, -1, :] / temperature\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        next_idx = torch.multinomial(probs, 1).item()\n",
    "\n",
    "        next_char = itos[next_idx]\n",
    "        tokens.append(next_idx)\n",
    "        generated.append(next_char)\n",
    "\n",
    "        # Stop at end-of-text token or double newline\n",
    "        if '<|endoftext|>' in ''.join(generated[-15:]):\n",
    "            break\n",
    "\n",
    "    response = ''.join(generated)\n",
    "\n",
    "    # Extract just the response part\n",
    "    if \"### Response:\" in response:\n",
    "        response = response.split(\"### Response:\")[-1].strip()\n",
    "        response = response.replace(\"<|endoftext|>\", \"\").strip()\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with training examples\n",
    "print(\"=\" * 60)\n",
    "print(\"TESTING ON TRAINING EXAMPLES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "test_questions = [\n",
    "    \"What is the capital of France?\",\n",
    "    \"What is 2 + 2?\",\n",
    "    \"Who wrote Romeo and Juliet?\",\n",
    "    \"Say hello.\",\n",
    "]\n",
    "\n",
    "for q in test_questions:\n",
    "    print(f\"\\nQ: {q}\")\n",
    "    print(f\"A: {ask(model, q)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test generalization (these weren't in training!)\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TESTING GENERALIZATION (not in training)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "novel_questions = [\n",
    "    \"What is the capital of Spain?\",  # Similar pattern, different answer\n",
    "    \"What is 3 + 3?\",                  # Similar pattern\n",
    "    \"Name three animals.\",             # Similar to \"Name three colors\"\n",
    "    \"Count to three.\",                 # Similar to \"Count to five\"\n",
    "]\n",
    "\n",
    "for q in novel_questions:\n",
    "    print(f\"\\nQ: {q}\")\n",
    "    print(f\"A: {ask(model, q)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generalization Limits\n",
    "\n",
    "With only 20 training examples, generalization is limited. Real instruction-tuned models like Alpaca use 52K examples, and models like GPT-3.5 use millions.\n",
    "\n",
    "However, even with few examples, the model learns the **format**: instruction → response pattern."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Understanding What Changed\n",
    "\n",
    "The key insight: **instruction tuning doesn't add new knowledge**—it teaches the model to **access and format** its knowledge appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see what happens WITHOUT instruction format\n",
    "@torch.no_grad()\n",
    "def complete(model, prompt, max_tokens=100, temperature=0.7):\n",
    "    \"\"\"Raw text completion without instruction formatting.\"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    tokens = [stoi.get(ch, 0) for ch in prompt]\n",
    "    generated = list(prompt)\n",
    "\n",
    "    for _ in range(max_tokens):\n",
    "        context = tokens[-block_size:]\n",
    "        x = torch.tensor([context]).to(device)\n",
    "\n",
    "        logits = model(x)[0, -1, :] / temperature\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        next_idx = torch.multinomial(probs, 1).item()\n",
    "\n",
    "        tokens.append(next_idx)\n",
    "        generated.append(itos[next_idx])\n",
    "\n",
    "    return ''.join(generated)\n",
    "\n",
    "print(\"Raw completion (no instruction format):\")\n",
    "print(\"-\" * 50)\n",
    "print(complete(model, \"What is the capital of France?\", max_tokens=80))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With instruction format, the model knows to:\n",
    "1. Look for \"### Instruction:\" marker\n",
    "2. Generate a direct response after \"### Response:\"\n",
    "3. Stop at the end-of-text token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model for Later Use\n",
    "\n",
    "Export the instruction-tuned model for use in Part 6 (DPO) and for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Create models directory\n",
    "os.makedirs('../models', exist_ok=True)\n",
    "\n",
    "# Save model checkpoint with architecture info\n",
    "checkpoint = {\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'model_config': {\n",
    "        'vocab_size': vocab_size,\n",
    "        'd_model': d_model,\n",
    "        'n_heads': n_heads,\n",
    "        'n_layers': n_layers,\n",
    "        'block_size': block_size,\n",
    "    },\n",
    "    'stoi': stoi,\n",
    "    'itos': itos,\n",
    "    'final_loss': losses[-1] if losses else None,\n",
    "}\n",
    "\n",
    "torch.save(checkpoint, '../models/instruction_tuned.pt')\n",
    "print(f\"Model saved to ../models/instruction_tuned.pt\")\n",
    "\n",
    "# Verify\n",
    "loaded = torch.load('../models/instruction_tuned.pt', weights_only=False)\n",
    "print(f\"Model config: {loaded['model_config']}\")\n",
    "print(f\"Model parameters: {num_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Up GPU Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "# Delete model and data tensors\n",
    "del model\n",
    "del X, Y\n",
    "\n",
    "# Clear CUDA cache\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "print(\"GPU memory cleared!\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU memory allocated: {torch.cuda.memory_allocated() / 1024**2:.1f} MB\")\n",
    "    print(f\"GPU memory cached: {torch.cuda.memory_reserved() / 1024**2:.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Real-World Instruction Tuning Pipeline\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────┐\n",
    "│                 INSTRUCTION TUNING PIPELINE                  │\n",
    "└─────────────────────────────────────────────────────────────┘\n",
    "\n",
    "1. PRETRAIN on massive text corpus\n",
    "   └── Model learns language patterns, facts, reasoning\n",
    "\n",
    "2. COLLECT instruction data\n",
    "   ├── Human-written (expensive, high quality)\n",
    "   ├── GPT-generated (Alpaca approach)\n",
    "   └── From existing NLP datasets\n",
    "\n",
    "3. FORMAT data consistently\n",
    "   └── Instruction: {task}\n",
    "       Response: {answer}\n",
    "\n",
    "4. FINE-TUNE on instruction data\n",
    "   ├── Usually 1-3 epochs\n",
    "   ├── Lower learning rate than pretraining\n",
    "   └── Careful not to forget pretrained knowledge\n",
    "\n",
    "5. EVALUATE\n",
    "   ├── Held-out instructions\n",
    "   ├── Human evaluation\n",
    "   └── Benchmark tasks\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "We implemented instruction tuning from scratch:\n",
    "\n",
    "| Aspect | What We Did |\n",
    "|--------|-------------|\n",
    "| Dataset | 20 instruction-response pairs |\n",
    "| Format | \"### Instruction:\\n{q}\\n\\n### Response:\\n{a}\" |\n",
    "| Model | Same transformer from Part 4 |\n",
    "| Training | Standard next-token prediction |\n",
    "| Result | Model follows instruction format |\n",
    "\n",
    "**Key insight**: Instruction tuning is just supervised fine-tuning on carefully formatted data. The magic is in the data, not the training procedure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's Next\n",
    "\n",
    "In **Part 6**, we'll add **DPO (Direct Preference Optimization)**—training the model to prefer good responses over bad ones. This is the final step toward alignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. **More data**: Add 50 more instruction-response pairs. How does quality improve?\n",
    "2. **Different formats**: Try \"User: {q}\\nAssistant: {a}\" format\n",
    "3. **Multi-turn**: Add conversation examples with multiple turns\n",
    "4. **Negative examples**: What if you include bad examples? (Spoiler: Part 6!)\n",
    "5. **LoRA**: Implement low-rank adaptation for efficient fine-tuning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nb-base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
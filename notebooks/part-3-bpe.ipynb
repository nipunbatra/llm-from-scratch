{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title-block-style: none\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: BPE Tokenizer from Scratch\n",
    "## From characters to subwords\n",
    "\n",
    "In Parts 1 and 2, we built character-level language models. They work, but have fundamental limitations:\n",
    "\n",
    "1. **No word understanding**: The model sees \"the\" as three separate decisions: t\u2192h\u2192e\n",
    "2. **Long sequences**: A sentence of 10 words might be 60+ characters\n",
    "3. **Inefficient learning**: Must learn spelling patterns from scratch\n",
    "\n",
    "Modern LLMs use **subword tokenization**, which finds a middle ground between characters and words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Byte-Pair Encoding (BPE)\n",
    "\n",
    "BPE is an algorithm that learns the most common character pairs and merges them. Repeat until you reach a target vocabulary size.\n",
    "\n",
    "**Example**:\n",
    "```\n",
    "Text: \"low lower lowest\"\n",
    "Initial: ['l', 'o', 'w', ' ', 'l', 'o', 'w', 'e', 'r', ' ', 'l', 'o', 'w', 'e', 's', 't']\n",
    "After merge 'l'+'o' \u2192 'lo': ['lo', 'w', ' ', 'lo', 'w', 'e', 'r', ' ', 'lo', 'w', 'e', 's', 't']\n",
    "After merge 'lo'+'w' \u2192 'low': ['low', ' ', 'low', 'e', 'r', ' ', 'low', 'e', 's', 't']\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-02T05:22:38.259224Z",
     "iopub.status.busy": "2026-01-02T05:22:38.258931Z",
     "iopub.status.idle": "2026-01-02T05:22:40.106444Z",
     "shell.execute_reply": "2026-01-02T05:22:40.105399Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import re\n",
    "import os\n",
    "import requests\n",
    "\n",
    "torch.manual_seed(42)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Implement BPE from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-02T05:22:40.109729Z",
     "iopub.status.busy": "2026-01-02T05:22:40.109430Z",
     "iopub.status.idle": "2026-01-02T05:22:40.120326Z",
     "shell.execute_reply": "2026-01-02T05:22:40.119468Z"
    }
   },
   "outputs": [],
   "source": [
    "class BPETokenizer:\n",
    "    \"\"\"\n",
    "    A from-scratch implementation of Byte-Pair Encoding.\n",
    "    \n",
    "    Algorithm:\n",
    "    1. Start with character-level vocabulary\n",
    "    2. Count all adjacent pairs in the data\n",
    "    3. Merge the most frequent pair into a new token\n",
    "    4. Repeat until desired vocabulary size\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size=256):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.merges = {}  # (token1, token2) -> new_token\n",
    "        self.vocab = {}   # token_id -> token_string\n",
    "        self.inverse_vocab = {}  # token_string -> token_id\n",
    "    \n",
    "    def _get_pairs(self, tokens):\n",
    "        \"\"\"Count frequency of adjacent pairs.\"\"\"\n",
    "        pairs = Counter()\n",
    "        for i in range(len(tokens) - 1):\n",
    "            pairs[(tokens[i], tokens[i + 1])] += 1\n",
    "        return pairs\n",
    "    \n",
    "    def _merge(self, tokens, pair, new_token):\n",
    "        \"\"\"Merge all occurrences of pair into new_token.\"\"\"\n",
    "        new_tokens = []\n",
    "        i = 0\n",
    "        while i < len(tokens):\n",
    "            if i < len(tokens) - 1 and (tokens[i], tokens[i + 1]) == pair:\n",
    "                new_tokens.append(new_token)\n",
    "                i += 2\n",
    "            else:\n",
    "                new_tokens.append(tokens[i])\n",
    "                i += 1\n",
    "        return new_tokens\n",
    "    \n",
    "    def train(self, text, verbose=True):\n",
    "        \"\"\"\n",
    "        Learn BPE merges from text.\n",
    "        \n",
    "        Args:\n",
    "            text: Training text\n",
    "            verbose: Print progress\n",
    "        \"\"\"\n",
    "        # Start with character-level tokens\n",
    "        tokens = list(text)\n",
    "        \n",
    "        # Initialize vocabulary with all unique characters\n",
    "        unique_chars = sorted(set(tokens))\n",
    "        for i, char in enumerate(unique_chars):\n",
    "            self.vocab[i] = char\n",
    "            self.inverse_vocab[char] = i\n",
    "        \n",
    "        next_id = len(unique_chars)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Initial vocabulary size: {len(unique_chars)}\")\n",
    "            print(f\"Target vocabulary size: {self.vocab_size}\")\n",
    "            print(f\"Merges to learn: {self.vocab_size - len(unique_chars)}\")\n",
    "            print(\"-\" * 50)\n",
    "        \n",
    "        # Learn merges\n",
    "        num_merges = self.vocab_size - len(unique_chars)\n",
    "        \n",
    "        for merge_idx in range(num_merges):\n",
    "            pairs = self._get_pairs(tokens)\n",
    "            \n",
    "            if not pairs:\n",
    "                if verbose:\n",
    "                    print(\"No more pairs to merge!\")\n",
    "                break\n",
    "            \n",
    "            # Find most frequent pair\n",
    "            best_pair = max(pairs, key=pairs.get)\n",
    "            best_count = pairs[best_pair]\n",
    "            \n",
    "            # Create new token by joining the pair\n",
    "            new_token = best_pair[0] + best_pair[1]\n",
    "            \n",
    "            # Record merge\n",
    "            self.merges[best_pair] = new_token\n",
    "            self.vocab[next_id] = new_token\n",
    "            self.inverse_vocab[new_token] = next_id\n",
    "            \n",
    "            # Apply merge\n",
    "            tokens = self._merge(tokens, best_pair, new_token)\n",
    "            next_id += 1\n",
    "            \n",
    "            if verbose and merge_idx % 50 == 0:\n",
    "                print(f\"Merge {merge_idx}: '{best_pair[0]}' + '{best_pair[1]}' \u2192 '{new_token}' (count: {best_count})\")\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"-\" * 50)\n",
    "            print(f\"Final vocabulary size: {len(self.vocab)}\")\n",
    "            print(f\"Compression: {len(text)} chars \u2192 {len(tokens)} tokens\")\n",
    "            print(f\"Compression ratio: {len(text) / len(tokens):.2f}x\")\n",
    "    \n",
    "    def encode(self, text):\n",
    "        \"\"\"\n",
    "        Encode text into token IDs.\n",
    "        \n",
    "        Apply learned merges in order of learning.\n",
    "        \"\"\"\n",
    "        tokens = list(text)\n",
    "        \n",
    "        # Apply each merge in order\n",
    "        for pair, new_token in self.merges.items():\n",
    "            tokens = self._merge(tokens, pair, new_token)\n",
    "        \n",
    "        # Convert to IDs\n",
    "        return [self.inverse_vocab[t] for t in tokens]\n",
    "    \n",
    "    def decode(self, ids):\n",
    "        \"\"\"Decode token IDs back to text.\"\"\"\n",
    "        return ''.join(self.vocab[i] for i in ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Train the Tokenizer\n",
    "\n",
    "Let's train on Shakespeare text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-02T05:22:40.122692Z",
     "iopub.status.busy": "2026-01-02T05:22:40.122552Z",
     "iopub.status.idle": "2026-01-02T05:22:40.127264Z",
     "shell.execute_reply": "2026-01-02T05:22:40.126553Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: 1,115,394 characters\n"
     ]
    }
   ],
   "source": [
    "# Load Shakespeare\n",
    "if not os.path.exists('shakespeare.txt'):\n",
    "    url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "    response = requests.get(url)\n",
    "    with open('shakespeare.txt', 'w') as f:\n",
    "        f.write(response.text)\n",
    "\n",
    "with open('shakespeare.txt', 'r') as f:\n",
    "    shakespeare = f.read()\n",
    "\n",
    "print(f\"Training data: {len(shakespeare):,} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-02T05:22:40.179946Z",
     "iopub.status.busy": "2026-01-02T05:22:40.179706Z",
     "iopub.status.idle": "2026-01-02T05:23:56.510443Z",
     "shell.execute_reply": "2026-01-02T05:23:56.497338Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial vocabulary size: 65\n",
      "Target vocabulary size: 300\n",
      "Merges to learn: 235\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merge 0: 'e' + ' ' \u2192 'e ' (count: 27643)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merge 50: 'l' + 'i' \u2192 'li' (count: 2358)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merge 100: 'w' + 'ith' \u2192 'with' (count: 1258)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merge 150: 'a' + 'y ' \u2192 'ay ' (count: 849)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merge 200: 's' + 'ha' \u2192 'sha' (count: 636)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Final vocabulary size: 300\n",
      "Compression: 1115394 chars \u2192 578590 tokens\n",
      "Compression ratio: 1.93x\n"
     ]
    }
   ],
   "source": [
    "# Train BPE tokenizer\n",
    "tokenizer = BPETokenizer(vocab_size=300)  # Start small\n",
    "tokenizer.train(shakespeare)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: See What It Learned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-02T05:23:56.514290Z",
     "iopub.status.busy": "2026-01-02T05:23:56.514042Z",
     "iopub.status.idle": "2026-01-02T05:23:56.520447Z",
     "shell.execute_reply": "2026-01-02T05:23:56.519519Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample vocabulary entries (sorted by length):\n",
      "--------------------------------------------------\n",
      "\n",
      "Shortest tokens (individual characters):\n",
      "  0: '\\n'\n",
      "  1: ' '\n",
      "  2: '!'\n",
      "  3: '$'\n",
      "  4: '&'\n",
      "  5: '''\n",
      "  6: ','\n",
      "  7: '-'\n",
      "  8: '.'\n",
      "  9: '3'\n",
      "\n",
      "Longest tokens (learned subwords):\n",
      "  247: 'e.\n",
      "\n",
      "'\n",
      "  271: 'thy '\n",
      "  277: ' the'\n",
      "  283: 'KING'\n",
      "  284: ' him'\n",
      "  292: 'But '\n",
      "  293: 'est '\n",
      "  295: 'but '\n",
      "  113: ' the '\n",
      "  166: 'have '\n",
      "  170: 'that '\n",
      "  202: 'ould '\n",
      "  243: ' and '\n",
      "  260: ' his '\n",
      "  268: 'That '\n",
      "  285: 'this '\n",
      "  299: ' with'\n",
      "  231: ',\n",
      "And '\n",
      "  286: ' this '\n",
      "  288: ' that '\n"
     ]
    }
   ],
   "source": [
    "# Look at the vocabulary\n",
    "print(\"Sample vocabulary entries (sorted by length):\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Sort by token length to see progression\n",
    "sorted_tokens = sorted(tokenizer.vocab.items(), key=lambda x: len(x[1]))\n",
    "\n",
    "# Show some short and long tokens\n",
    "print(\"\\nShortest tokens (individual characters):\")\n",
    "for idx, token in sorted_tokens[:10]:\n",
    "    print(f\"  {idx}: '{repr(token)[1:-1]}'\")\n",
    "\n",
    "print(\"\\nLongest tokens (learned subwords):\")\n",
    "for idx, token in sorted_tokens[-20:]:\n",
    "    print(f\"  {idx}: '{token}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-02T05:23:56.522156Z",
     "iopub.status.busy": "2026-01-02T05:23:56.521937Z",
     "iopub.status.idle": "2026-01-02T05:23:56.528685Z",
     "shell.execute_reply": "2026-01-02T05:23:56.527917Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding examples:\n",
      "============================================================\n",
      "\n",
      "Original: 'To be or not to be'\n",
      "Token IDs: [227, 197, 77, 1, 136, 92, 178]\n",
      "Tokens: ['To ', 'be ', 'or', ' ', 'not ', 'to ', 'be']\n",
      "Decoded: 'To be or not to be'\n",
      "Compression: 18 chars \u2192 7 tokens\n",
      "\n",
      "Original: 'ROMEO: Wherefore art thou'\n",
      "Token IDs: [30, 27, 25, 17, 27, 10, 1, 169, 72, 43, 109, 65, 81, 67, 185]\n",
      "Tokens: ['R', 'O', 'M', 'E', 'O', ':', ' ', 'Wh', 'er', 'e', 'for', 'e ', 'ar', 't ', 'thou']\n",
      "Decoded: 'ROMEO: Wherefore art thou'\n",
      "Compression: 25 chars \u2192 15 tokens\n",
      "\n",
      "Original: 'the king'\n",
      "Token IDs: [104, 49, 100]\n",
      "Tokens: ['the ', 'k', 'ing']\n",
      "Decoded: 'the king'\n",
      "Compression: 8 chars \u2192 3 tokens\n"
     ]
    }
   ],
   "source": [
    "# Test encoding\n",
    "test_texts = [\n",
    "    \"To be or not to be\",\n",
    "    \"ROMEO: Wherefore art thou\",\n",
    "    \"the king\",\n",
    "]\n",
    "\n",
    "print(\"Encoding examples:\")\n",
    "print(\"=\" * 60)\n",
    "for text in test_texts:\n",
    "    ids = tokenizer.encode(text)\n",
    "    decoded = tokenizer.decode(ids)\n",
    "    \n",
    "    print(f\"\\nOriginal: '{text}'\")\n",
    "    print(f\"Token IDs: {ids}\")\n",
    "    print(f\"Tokens: {[tokenizer.vocab[i] for i in ids]}\")\n",
    "    print(f\"Decoded: '{decoded}'\")\n",
    "    print(f\"Compression: {len(text)} chars \u2192 {len(ids)} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What BPE Learns\n",
    "\n",
    "Common patterns in Shakespeare:\n",
    "- \"the\", \"and\", \"to\" become single tokens\n",
    "- Common suffixes like \"ing\", \"ed\", \"er\"\n",
    "- Character names like \"ROMEO\" might merge partially\n",
    "\n",
    "The tokenizer learns the **statistical structure** of the language!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Build Language Model with BPE\n",
    "\n",
    "Now let's train a language model using our BPE tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-02T05:23:56.530937Z",
     "iopub.status.busy": "2026-01-02T05:23:56.530720Z",
     "iopub.status.idle": "2026-01-02T05:24:26.789909Z",
     "shell.execute_reply": "2026-01-02T05:24:26.788932Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: 1,115,394 characters\n",
      "Encoded: 578,590 tokens\n",
      "Compression ratio: 1.93x\n"
     ]
    }
   ],
   "source": [
    "# Encode the entire text\n",
    "encoded = tokenizer.encode(shakespeare)\n",
    "print(f\"Original: {len(shakespeare):,} characters\")\n",
    "print(f\"Encoded: {len(encoded):,} tokens\")\n",
    "print(f\"Compression ratio: {len(shakespeare) / len(encoded):.2f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-02T05:24:26.792522Z",
     "iopub.status.busy": "2026-01-02T05:24:26.792356Z",
     "iopub.status.idle": "2026-01-02T05:24:30.771216Z",
     "shell.execute_reply": "2026-01-02T05:24:30.770287Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 578,558 examples\n"
     ]
    }
   ],
   "source": [
    "# Create training dataset\n",
    "block_size = 32  # Now 32 TOKENS, not characters\n",
    "\n",
    "def build_dataset_bpe(encoded_text, block_size):\n",
    "    X, Y = [], []\n",
    "    for i in range(len(encoded_text) - block_size):\n",
    "        X.append(encoded_text[i:i + block_size])\n",
    "        Y.append(encoded_text[i + block_size])\n",
    "    return torch.tensor(X), torch.tensor(Y)\n",
    "\n",
    "X, Y = build_dataset_bpe(encoded, block_size)\n",
    "print(f\"Dataset size: {len(X):,} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-02T05:24:30.773866Z",
     "iopub.status.busy": "2026-01-02T05:24:30.773699Z",
     "iopub.status.idle": "2026-01-02T05:24:31.038161Z",
     "shell.execute_reply": "2026-01-02T05:24:31.037126Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 1,222,188\n",
      "\n",
      "--- Model Scale Context ---\n",
      "Our model:     ~1222K parameters\n",
      "GPT-2 Small:   124M parameters  (101x larger)\n",
      "GPT-3:         175B parameters  (143186x larger)\n",
      "Claude/GPT-4:  ~1T+ parameters  (818205x larger)\n"
     ]
    }
   ],
   "source": [
    "# Same model architecture!\n",
    "class TokenLM(nn.Module):\n",
    "    \"\"\"Language model operating on BPE tokens.\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, block_size, emb_dim, hidden_size):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.hidden = nn.Linear(block_size * emb_dim, hidden_size)\n",
    "        self.output = nn.Linear(hidden_size, vocab_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.emb(x)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = torch.tanh(self.hidden(x))\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "# Create model with larger capacity\n",
    "vocab_size = len(tokenizer.vocab)\n",
    "emb_dim = 64   # Increased from 32\n",
    "hidden_size = 512  # Increased from 256\n",
    "\n",
    "model = TokenLM(vocab_size, block_size, emb_dim, hidden_size).to(device)\n",
    "\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Model parameters: {num_params:,}\")\n",
    "\n",
    "# Model scale context\n",
    "print(f\"\\n--- Model Scale Context ---\")\n",
    "print(f\"Our model:     ~{num_params/1000:.0f}K parameters\")\n",
    "print(f\"GPT-2 Small:   124M parameters  ({124_000_000/num_params:.0f}x larger)\")\n",
    "print(f\"GPT-3:         175B parameters  ({175_000_000_000/num_params:.0f}x larger)\")\n",
    "print(f\"Claude/GPT-4:  ~1T+ parameters  ({1_000_000_000_000/num_params:.0f}x larger)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-02T05:24:31.040286Z",
     "iopub.status.busy": "2026-01-02T05:24:31.039986Z",
     "iopub.status.idle": "2026-01-02T05:29:30.689816Z",
     "shell.execute_reply": "2026-01-02T05:29:30.688555Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss = 4.3560\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100: Loss = 0.2567\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 200: Loss = 0.0432\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 300: Loss = 0.0364\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 400: Loss = 0.0320\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "def train(model, X, Y, epochs=500, batch_size=2048, lr=0.001):\n",
    "    model.train()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    \n",
    "    X, Y = X.to(device), Y.to(device)\n",
    "    losses = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        perm = torch.randperm(X.shape[0])\n",
    "        total_loss, n_batches = 0, 0\n",
    "        \n",
    "        for i in range(0, X.shape[0], batch_size):\n",
    "            idx = perm[i:i+batch_size]\n",
    "            logits = model(X[idx])\n",
    "            loss = loss_fn(logits, Y[idx])\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            n_batches += 1\n",
    "        \n",
    "        losses.append(total_loss / n_batches)\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch}: Loss = {losses[-1]:.4f}\")\n",
    "    \n",
    "    return losses\n",
    "\n",
    "losses = train(model, X, Y, epochs=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Generate with BPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-02T05:29:30.692681Z",
     "iopub.status.busy": "2026-01-02T05:29:30.692012Z",
     "iopub.status.idle": "2026-01-02T05:29:30.815313Z",
     "shell.execute_reply": "2026-01-02T05:29:30.814175Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "GENERATED TEXT (BPE Tokenization)\n",
      "============================================================\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ROMEO:\n",
      "why thine in you woul\n",
      "\n",
      "s\n",
      "On the inientans. Come, if you beor paice\n",
      "Dodest but by get at I a wronge: proclaim:\n",
      "And leave me to be evenge, I have drans:\n",
      "Comarurious,\n",
      "And, if you may appos. Bomingbatch herce!\n",
      "\n",
      "ANREBIO:\n",
      "Tell, moll hear you; made intwent\n",
      "O, 'tood but joath. Thushy stafe, being?\n",
      "\n",
      "M\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def generate_bpe(model, tokenizer, seed_text, length=200, temperature=0.8):\n",
    "    model.eval()\n",
    "    \n",
    "    # Encode seed\n",
    "    tokens = tokenizer.encode(seed_text)\n",
    "    \n",
    "    # Pad or truncate to block_size\n",
    "    if len(tokens) < block_size:\n",
    "        tokens = [0] * (block_size - len(tokens)) + tokens\n",
    "    else:\n",
    "        tokens = tokens[-block_size:]\n",
    "    \n",
    "    generated_ids = list(tokens)\n",
    "    \n",
    "    for _ in range(length):\n",
    "        x = torch.tensor([tokens[-block_size:]]).to(device)\n",
    "        logits = model(x)\n",
    "        probs = F.softmax(logits / temperature, dim=-1)\n",
    "        next_id = torch.multinomial(probs, 1).item()\n",
    "        \n",
    "        generated_ids.append(next_id)\n",
    "        tokens = tokens[1:] + [next_id]\n",
    "    \n",
    "    return tokenizer.decode(generated_ids)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"GENERATED TEXT (BPE Tokenization)\")\n",
    "print(\"=\" * 60)\n",
    "print(generate_bpe(model, tokenizer, \"ROMEO:\\n\", length=150))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison: Character vs BPE\n",
    "\n",
    "| Aspect | Character-Level | BPE (vocab=300) |\n",
    "|--------|-----------------|-----------------|  \n",
    "| Vocab size | 65 | 300 |\n",
    "| Sequence for \"the king\" | 8 tokens | ~3-4 tokens |\n",
    "| Context window (32 tokens) | 32 characters | ~100+ characters |\n",
    "| Compression ratio | 1x | ~3-4x |\n",
    "| Learns word boundaries | No | Partially |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why BPE is Better\n",
    "\n",
    "1. **Efficiency**: Fewer tokens = longer effective context\n",
    "2. **Semantics**: Whole words/subwords carry more meaning\n",
    "3. **Generalization**: Handles rare words by splitting into known subwords\n",
    "4. **Scalability**: How GPT-2/3/4 and modern LLMs tokenize\n",
    "\n",
    "A 32-token context with BPE sees ~100 characters, vs only 32 with character-level!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How Modern Tokenizers Extend This\n",
    "\n",
    "GPT-2/3 use a variant called **Byte-level BPE**:\n",
    "\n",
    "1. Start with 256 byte values (not characters)\n",
    "2. Can represent ANY text (Unicode, emojis, etc.)\n",
    "3. Vocabulary size ~50,000 tokens\n",
    "\n",
    "Our implementation is simplified but captures the core algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train on Names with BPE\n",
    "\n",
    "Let's also test BPE on names to compare:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-02T05:29:30.817993Z",
     "iopub.status.busy": "2026-01-02T05:29:30.817849Z",
     "iopub.status.idle": "2026-01-02T05:29:31.055200Z",
     "shell.execute_reply": "2026-01-02T05:29:31.054130Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Names corpus: 44,324 characters\n"
     ]
    }
   ],
   "source": [
    "# Download names if needed\n",
    "if not os.path.exists('names.csv'):\n",
    "    url = \"https://raw.githubusercontent.com/balasahebgulave/Dataset-Indian-Names/master/Indian_Names.csv\"\n",
    "    response = requests.get(url)\n",
    "    with open('names.csv', 'w') as f:\n",
    "        f.write(response.text)\n",
    "\n",
    "import pandas as pd\n",
    "names = pd.read_csv('names.csv')[\"Name\"].str.lower().str.strip()\n",
    "names = names[names.str.len().between(3, 9)]\n",
    "names = names[names.apply(lambda x: str(x).isalpha())]\n",
    "names_text = '\\n'.join(names.tolist())\n",
    "\n",
    "print(f\"Names corpus: {len(names_text):,} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-02T05:29:31.057655Z",
     "iopub.status.busy": "2026-01-02T05:29:31.057412Z",
     "iopub.status.idle": "2026-01-02T05:29:32.119974Z",
     "shell.execute_reply": "2026-01-02T05:29:32.118977Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial vocabulary size: 27\n",
      "Target vocabulary size: 100\n",
      "Merges to learn: 73\n",
      "--------------------------------------------------\n",
      "Merge 0: 'a' + '\n",
      "' \u2192 'a\n",
      "' (count: 1510)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merge 50: 'ee' + 't' \u2192 'eet' (count: 112)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Final vocabulary size: 100\n",
      "Compression: 44324 chars \u2192 26636 tokens\n",
      "Compression ratio: 1.66x\n"
     ]
    }
   ],
   "source": [
    "# Train smaller BPE for names\n",
    "names_tokenizer = BPETokenizer(vocab_size=100)\n",
    "names_tokenizer.train(names_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-02T05:29:32.122200Z",
     "iopub.status.busy": "2026-01-02T05:29:32.122048Z",
     "iopub.status.idle": "2026-01-02T05:29:32.126181Z",
     "shell.execute_reply": "2026-01-02T05:29:32.125249Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Name encoding examples:\n",
      "  nipun \u2192 ['n', 'i', 'p', 'un']\n",
      "  priya \u2192 ['p', 'r', 'i', 'y', 'a']\n",
      "  arjun \u2192 ['ar', 'j', 'un']\n",
      "  krishna \u2192 ['k', 'r', 'ish', 'n', 'a']\n"
     ]
    }
   ],
   "source": [
    "# Test encoding names\n",
    "test_names = [\"nipun\", \"priya\", \"arjun\", \"krishna\"]\n",
    "print(\"\\nName encoding examples:\")\n",
    "for name in test_names:\n",
    "    ids = names_tokenizer.encode(name)\n",
    "    tokens = [names_tokenizer.vocab[i] for i in ids]\n",
    "    print(f\"  {name} \u2192 {tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model and Tokenizer\n",
    "\n",
    "Export both the BPE model and tokenizer for later use and comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-02T05:29:32.128304Z",
     "iopub.status.busy": "2026-01-02T05:29:32.128086Z",
     "iopub.status.idle": "2026-01-02T05:29:32.146301Z",
     "shell.execute_reply": "2026-01-02T05:29:32.145278Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer saved to ../models/bpe_tokenizer_shakespeare.pkl\n",
      "Model saved to ../models/bpe_lm_shakespeare.pt\n",
      "\n",
      "Tokenizer vocab size: 300\n",
      "Model parameters: 1,222,188\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Create models directory\n",
    "os.makedirs('../models', exist_ok=True)\n",
    "\n",
    "# Save BPE tokenizer\n",
    "tokenizer_data = {\n",
    "    'vocab_size': len(tokenizer.vocab),\n",
    "    'merges': tokenizer.merges,\n",
    "    'vocab': tokenizer.vocab,\n",
    "    'inverse_vocab': tokenizer.inverse_vocab,\n",
    "}\n",
    "with open('../models/bpe_tokenizer_shakespeare.pkl', 'wb') as f:\n",
    "    pickle.dump(tokenizer_data, f)\n",
    "print(f\"Tokenizer saved to ../models/bpe_tokenizer_shakespeare.pkl\")\n",
    "\n",
    "# Save model checkpoint\n",
    "checkpoint = {\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'model_config': {\n",
    "        'vocab_size': vocab_size,\n",
    "        'block_size': block_size,\n",
    "        'emb_dim': emb_dim,\n",
    "        'hidden_size': hidden_size,\n",
    "    },\n",
    "    'final_loss': losses[-1] if losses else None,\n",
    "}\n",
    "torch.save(checkpoint, '../models/bpe_lm_shakespeare.pt')\n",
    "print(f\"Model saved to ../models/bpe_lm_shakespeare.pt\")\n",
    "\n",
    "# Verify\n",
    "print(f\"\\nTokenizer vocab size: {len(tokenizer.vocab)}\")\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Up GPU Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-02T05:29:32.148563Z",
     "iopub.status.busy": "2026-01-02T05:29:32.148424Z",
     "iopub.status.idle": "2026-01-02T05:29:32.339086Z",
     "shell.execute_reply": "2026-01-02T05:29:32.338302Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory cleared!\n",
      "GPU memory allocated: 20.9 MB\n",
      "GPU memory cached: 62.0 MB\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "# Delete model and data tensors\n",
    "del model\n",
    "del X, Y\n",
    "\n",
    "# Clear CUDA cache\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "print(\"GPU memory cleared!\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU memory allocated: {torch.cuda.memory_allocated() / 1024**2:.1f} MB\")\n",
    "    print(f\"GPU memory cached: {torch.cuda.memory_reserved() / 1024**2:.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this part, we built **BPE tokenization from scratch**:\n",
    "\n",
    "1. **Algorithm**: Iteratively merge most common pairs\n",
    "2. **Vocabulary**: Learns subword units from data\n",
    "3. **Compression**: 3-4x fewer tokens than characters\n",
    "4. **Longer context**: Same number of tokens sees more text\n",
    "\n",
    "| Step | What happens |\n",
    "|------|--------------|\n",
    "| Initialize | Start with characters |\n",
    "| Count pairs | Find adjacent token frequencies |\n",
    "| Merge best | Create new token from pair |\n",
    "| Repeat | Until target vocab size |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's Next\n",
    "\n",
    "In **Part 4**, we'll add **self-attention**\u2014the key mechanism that makes transformers powerful. Our MLP treats all context positions equally; attention lets the model focus on relevant parts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. **Vary vocab size**: Try 100, 500, 1000. How does compression change?\n",
    "2. **Different data**: Train BPE on Python code or another language\n",
    "3. **Subword analysis**: What common subwords does it learn?\n",
    "4. **Byte-level BPE**: Modify to work with bytes instead of characters\n",
    "5. **OOV handling**: What happens with words not in training data?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nb-base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
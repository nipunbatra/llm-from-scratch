{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: BPE Tokenizer from Scratch\n",
    "## From characters to subwords\n",
    "\n",
    "In Parts 1 and 2, we built character-level language models. They work, but have fundamental limitations:\n",
    "\n",
    "1. **No word understanding**: The model sees \"the\" as three separate decisions: t→h→e\n",
    "2. **Long sequences**: A sentence of 10 words might be 60+ characters\n",
    "3. **Inefficient learning**: Must learn spelling patterns from scratch\n",
    "\n",
    "Modern LLMs use **subword tokenization**, which finds a middle ground between characters and words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Byte-Pair Encoding (BPE)\n",
    "\n",
    "BPE is an algorithm that learns the most common character pairs and merges them. Repeat until you reach a target vocabulary size.\n",
    "\n",
    "**Example**:\n",
    "```\n",
    "Text: \"low lower lowest\"\n",
    "Initial: ['l', 'o', 'w', ' ', 'l', 'o', 'w', 'e', 'r', ' ', 'l', 'o', 'w', 'e', 's', 't']\n",
    "After merge 'l'+'o' → 'lo': ['lo', 'w', ' ', 'lo', 'w', 'e', 'r', ' ', 'lo', 'w', 'e', 's', 't']\n",
    "After merge 'lo'+'w' → 'low': ['low', ' ', 'low', 'e', 'r', ' ', 'low', 'e', 's', 't']\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import re\n",
    "import os\n",
    "import requests\n",
    "\n",
    "torch.manual_seed(42)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Implement BPE from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BPETokenizer:\n",
    "    \"\"\"\n",
    "    A from-scratch implementation of Byte-Pair Encoding.\n",
    "    \n",
    "    Algorithm:\n",
    "    1. Start with character-level vocabulary\n",
    "    2. Count all adjacent pairs in the data\n",
    "    3. Merge the most frequent pair into a new token\n",
    "    4. Repeat until desired vocabulary size\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size=256):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.merges = {}  # (token1, token2) -> new_token\n",
    "        self.vocab = {}   # token_id -> token_string\n",
    "        self.inverse_vocab = {}  # token_string -> token_id\n",
    "    \n",
    "    def _get_pairs(self, tokens):\n",
    "        \"\"\"Count frequency of adjacent pairs.\"\"\"\n",
    "        pairs = Counter()\n",
    "        for i in range(len(tokens) - 1):\n",
    "            pairs[(tokens[i], tokens[i + 1])] += 1\n",
    "        return pairs\n",
    "    \n",
    "    def _merge(self, tokens, pair, new_token):\n",
    "        \"\"\"Merge all occurrences of pair into new_token.\"\"\"\n",
    "        new_tokens = []\n",
    "        i = 0\n",
    "        while i < len(tokens):\n",
    "            if i < len(tokens) - 1 and (tokens[i], tokens[i + 1]) == pair:\n",
    "                new_tokens.append(new_token)\n",
    "                i += 2\n",
    "            else:\n",
    "                new_tokens.append(tokens[i])\n",
    "                i += 1\n",
    "        return new_tokens\n",
    "    \n",
    "    def train(self, text, verbose=True):\n",
    "        \"\"\"\n",
    "        Learn BPE merges from text.\n",
    "        \n",
    "        Args:\n",
    "            text: Training text\n",
    "            verbose: Print progress\n",
    "        \"\"\"\n",
    "        # Start with character-level tokens\n",
    "        tokens = list(text)\n",
    "        \n",
    "        # Initialize vocabulary with all unique characters\n",
    "        unique_chars = sorted(set(tokens))\n",
    "        for i, char in enumerate(unique_chars):\n",
    "            self.vocab[i] = char\n",
    "            self.inverse_vocab[char] = i\n",
    "        \n",
    "        next_id = len(unique_chars)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Initial vocabulary size: {len(unique_chars)}\")\n",
    "            print(f\"Target vocabulary size: {self.vocab_size}\")\n",
    "            print(f\"Merges to learn: {self.vocab_size - len(unique_chars)}\")\n",
    "            print(\"-\" * 50)\n",
    "        \n",
    "        # Learn merges\n",
    "        num_merges = self.vocab_size - len(unique_chars)\n",
    "        \n",
    "        for merge_idx in range(num_merges):\n",
    "            pairs = self._get_pairs(tokens)\n",
    "            \n",
    "            if not pairs:\n",
    "                if verbose:\n",
    "                    print(\"No more pairs to merge!\")\n",
    "                break\n",
    "            \n",
    "            # Find most frequent pair\n",
    "            best_pair = max(pairs, key=pairs.get)\n",
    "            best_count = pairs[best_pair]\n",
    "            \n",
    "            # Create new token by joining the pair\n",
    "            new_token = best_pair[0] + best_pair[1]\n",
    "            \n",
    "            # Record merge\n",
    "            self.merges[best_pair] = new_token\n",
    "            self.vocab[next_id] = new_token\n",
    "            self.inverse_vocab[new_token] = next_id\n",
    "            \n",
    "            # Apply merge\n",
    "            tokens = self._merge(tokens, best_pair, new_token)\n",
    "            next_id += 1\n",
    "            \n",
    "            if verbose and merge_idx % 50 == 0:\n",
    "                print(f\"Merge {merge_idx}: '{best_pair[0]}' + '{best_pair[1]}' → '{new_token}' (count: {best_count})\")\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"-\" * 50)\n",
    "            print(f\"Final vocabulary size: {len(self.vocab)}\")\n",
    "            print(f\"Compression: {len(text)} chars → {len(tokens)} tokens\")\n",
    "            print(f\"Compression ratio: {len(text) / len(tokens):.2f}x\")\n",
    "    \n",
    "    def encode(self, text):\n",
    "        \"\"\"\n",
    "        Encode text into token IDs.\n",
    "        \n",
    "        Apply learned merges in order of learning.\n",
    "        \"\"\"\n",
    "        tokens = list(text)\n",
    "        \n",
    "        # Apply each merge in order\n",
    "        for pair, new_token in self.merges.items():\n",
    "            tokens = self._merge(tokens, pair, new_token)\n",
    "        \n",
    "        # Convert to IDs\n",
    "        return [self.inverse_vocab[t] for t in tokens]\n",
    "    \n",
    "    def decode(self, ids):\n",
    "        \"\"\"Decode token IDs back to text.\"\"\"\n",
    "        return ''.join(self.vocab[i] for i in ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Train the Tokenizer\n",
    "\n",
    "Let's train on Shakespeare text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Shakespeare\n",
    "if not os.path.exists('shakespeare.txt'):\n",
    "    url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "    response = requests.get(url)\n",
    "    with open('shakespeare.txt', 'w') as f:\n",
    "        f.write(response.text)\n",
    "\n",
    "with open('shakespeare.txt', 'r') as f:\n",
    "    shakespeare = f.read()\n",
    "\n",
    "print(f\"Training data: {len(shakespeare):,} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train BPE tokenizer\n",
    "tokenizer = BPETokenizer(vocab_size=300)  # Start small\n",
    "tokenizer.train(shakespeare)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: See What It Learned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the vocabulary\n",
    "print(\"Sample vocabulary entries (sorted by length):\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Sort by token length to see progression\n",
    "sorted_tokens = sorted(tokenizer.vocab.items(), key=lambda x: len(x[1]))\n",
    "\n",
    "# Show some short and long tokens\n",
    "print(\"\\nShortest tokens (individual characters):\")\n",
    "for idx, token in sorted_tokens[:10]:\n",
    "    print(f\"  {idx}: '{repr(token)[1:-1]}'\")\n",
    "\n",
    "print(\"\\nLongest tokens (learned subwords):\")\n",
    "for idx, token in sorted_tokens[-20:]:\n",
    "    print(f\"  {idx}: '{token}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test encoding\n",
    "test_texts = [\n",
    "    \"To be or not to be\",\n",
    "    \"ROMEO: Wherefore art thou\",\n",
    "    \"the king\",\n",
    "]\n",
    "\n",
    "print(\"Encoding examples:\")\n",
    "print(\"=\" * 60)\n",
    "for text in test_texts:\n",
    "    ids = tokenizer.encode(text)\n",
    "    decoded = tokenizer.decode(ids)\n",
    "    \n",
    "    print(f\"\\nOriginal: '{text}'\")\n",
    "    print(f\"Token IDs: {ids}\")\n",
    "    print(f\"Tokens: {[tokenizer.vocab[i] for i in ids]}\")\n",
    "    print(f\"Decoded: '{decoded}'\")\n",
    "    print(f\"Compression: {len(text)} chars → {len(ids)} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What BPE Learns\n",
    "\n",
    "Common patterns in Shakespeare:\n",
    "- \"the\", \"and\", \"to\" become single tokens\n",
    "- Common suffixes like \"ing\", \"ed\", \"er\"\n",
    "- Character names like \"ROMEO\" might merge partially\n",
    "\n",
    "The tokenizer learns the **statistical structure** of the language!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Build Language Model with BPE\n",
    "\n",
    "Now let's train a language model using our BPE tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the entire text\n",
    "encoded = tokenizer.encode(shakespeare)\n",
    "print(f\"Original: {len(shakespeare):,} characters\")\n",
    "print(f\"Encoded: {len(encoded):,} tokens\")\n",
    "print(f\"Compression ratio: {len(shakespeare) / len(encoded):.2f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training dataset\n",
    "block_size = 32  # Now 32 TOKENS, not characters\n",
    "\n",
    "def build_dataset_bpe(encoded_text, block_size):\n",
    "    X, Y = [], []\n",
    "    for i in range(len(encoded_text) - block_size):\n",
    "        X.append(encoded_text[i:i + block_size])\n",
    "        Y.append(encoded_text[i + block_size])\n",
    "    return torch.tensor(X), torch.tensor(Y)\n",
    "\n",
    "X, Y = build_dataset_bpe(encoded, block_size)\n",
    "print(f\"Dataset size: {len(X):,} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same model architecture!\n",
    "class TokenLM(nn.Module):\n",
    "    \"\"\"Language model operating on BPE tokens.\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, block_size, emb_dim, hidden_size):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.hidden = nn.Linear(block_size * emb_dim, hidden_size)\n",
    "        self.output = nn.Linear(hidden_size, vocab_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.emb(x)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = torch.tanh(self.hidden(x))\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "# Create model\n",
    "vocab_size = len(tokenizer.vocab)\n",
    "emb_dim = 32\n",
    "hidden_size = 256\n",
    "\n",
    "model = TokenLM(vocab_size, block_size, emb_dim, hidden_size).to(device)\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "def train(model, X, Y, epochs=500, batch_size=2048, lr=0.001):\n",
    "    model.train()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    \n",
    "    X, Y = X.to(device), Y.to(device)\n",
    "    losses = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        perm = torch.randperm(X.shape[0])\n",
    "        total_loss, n_batches = 0, 0\n",
    "        \n",
    "        for i in range(0, X.shape[0], batch_size):\n",
    "            idx = perm[i:i+batch_size]\n",
    "            logits = model(X[idx])\n",
    "            loss = loss_fn(logits, Y[idx])\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            n_batches += 1\n",
    "        \n",
    "        losses.append(total_loss / n_batches)\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch}: Loss = {losses[-1]:.4f}\")\n",
    "    \n",
    "    return losses\n",
    "\n",
    "losses = train(model, X, Y, epochs=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Generate with BPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate_bpe(model, tokenizer, seed_text, length=200, temperature=0.8):\n",
    "    model.eval()\n",
    "    \n",
    "    # Encode seed\n",
    "    tokens = tokenizer.encode(seed_text)\n",
    "    \n",
    "    # Pad or truncate to block_size\n",
    "    if len(tokens) < block_size:\n",
    "        tokens = [0] * (block_size - len(tokens)) + tokens\n",
    "    else:\n",
    "        tokens = tokens[-block_size:]\n",
    "    \n",
    "    generated_ids = list(tokens)\n",
    "    \n",
    "    for _ in range(length):\n",
    "        x = torch.tensor([tokens[-block_size:]]).to(device)\n",
    "        logits = model(x)\n",
    "        probs = F.softmax(logits / temperature, dim=-1)\n",
    "        next_id = torch.multinomial(probs, 1).item()\n",
    "        \n",
    "        generated_ids.append(next_id)\n",
    "        tokens = tokens[1:] + [next_id]\n",
    "    \n",
    "    return tokenizer.decode(generated_ids)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"GENERATED TEXT (BPE Tokenization)\")\n",
    "print(\"=\" * 60)\n",
    "print(generate_bpe(model, tokenizer, \"ROMEO:\\n\", length=150))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison: Character vs BPE\n",
    "\n",
    "| Aspect | Character-Level | BPE (vocab=300) |\n",
    "|--------|-----------------|-----------------|  \n",
    "| Vocab size | 65 | 300 |\n",
    "| Sequence for \"the king\" | 8 tokens | ~3-4 tokens |\n",
    "| Context window (32 tokens) | 32 characters | ~100+ characters |\n",
    "| Compression ratio | 1x | ~3-4x |\n",
    "| Learns word boundaries | No | Partially |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why BPE is Better\n",
    "\n",
    "1. **Efficiency**: Fewer tokens = longer effective context\n",
    "2. **Semantics**: Whole words/subwords carry more meaning\n",
    "3. **Generalization**: Handles rare words by splitting into known subwords\n",
    "4. **Scalability**: How GPT-2/3/4 and modern LLMs tokenize\n",
    "\n",
    "A 32-token context with BPE sees ~100 characters, vs only 32 with character-level!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How Modern Tokenizers Extend This\n",
    "\n",
    "GPT-2/3 use a variant called **Byte-level BPE**:\n",
    "\n",
    "1. Start with 256 byte values (not characters)\n",
    "2. Can represent ANY text (Unicode, emojis, etc.)\n",
    "3. Vocabulary size ~50,000 tokens\n",
    "\n",
    "Our implementation is simplified but captures the core algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train on Names with BPE\n",
    "\n",
    "Let's also test BPE on names to compare:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download names if needed\n",
    "if not os.path.exists('names.csv'):\n",
    "    url = \"https://raw.githubusercontent.com/balasahebgulave/Dataset-Indian-Names/master/Indian_Names.csv\"\n",
    "    response = requests.get(url)\n",
    "    with open('names.csv', 'w') as f:\n",
    "        f.write(response.text)\n",
    "\n",
    "import pandas as pd\n",
    "names = pd.read_csv('names.csv')[\"Name\"].str.lower().str.strip()\n",
    "names = names[names.str.len().between(3, 9)]\n",
    "names = names[names.apply(lambda x: str(x).isalpha())]\n",
    "names_text = '\\n'.join(names.tolist())\n",
    "\n",
    "print(f\"Names corpus: {len(names_text):,} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train smaller BPE for names\n",
    "names_tokenizer = BPETokenizer(vocab_size=100)\n",
    "names_tokenizer.train(names_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test encoding names\n",
    "test_names = [\"nipun\", \"priya\", \"arjun\", \"krishna\"]\n",
    "print(\"\\nName encoding examples:\")\n",
    "for name in test_names:\n",
    "    ids = names_tokenizer.encode(name)\n",
    "    tokens = [names_tokenizer.vocab[i] for i in ids]\n",
    "    print(f\"  {name} → {tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this part, we built **BPE tokenization from scratch**:\n",
    "\n",
    "1. **Algorithm**: Iteratively merge most common pairs\n",
    "2. **Vocabulary**: Learns subword units from data\n",
    "3. **Compression**: 3-4x fewer tokens than characters\n",
    "4. **Longer context**: Same number of tokens sees more text\n",
    "\n",
    "| Step | What happens |\n",
    "|------|--------------|\n",
    "| Initialize | Start with characters |\n",
    "| Count pairs | Find adjacent token frequencies |\n",
    "| Merge best | Create new token from pair |\n",
    "| Repeat | Until target vocab size |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's Next\n",
    "\n",
    "In **Part 4**, we'll add **self-attention**—the key mechanism that makes transformers powerful. Our MLP treats all context positions equally; attention lets the model focus on relevant parts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. **Vary vocab size**: Try 100, 500, 1000. How does compression change?\n",
    "2. **Different data**: Train BPE on Python code or another language\n",
    "3. **Subword analysis**: What common subwords does it learn?\n",
    "4. **Byte-level BPE**: Modify to work with bytes instead of characters\n",
    "5. **OOV handling**: What happens with words not in training data?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

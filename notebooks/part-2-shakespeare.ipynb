{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Shakespeare Pretraining\n",
    "## Same model, different domain\n",
    "\n",
    "In Part 1, we built a character-level language model that generates Indian names. Now we'll prove that **the same architecture works on completely different data**.\n",
    "\n",
    "We'll train on Shakespeare's text and generate new \"Shakespeare-like\" prose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Key Insight\n",
    "\n",
    "Language models are **general-purpose pattern learners**. The same architecture that learned:\n",
    "\n",
    "- Names often start with consonants\n",
    "- Vowels follow consonants in patterns\n",
    "- Names end with 'a', 'i', or consonants\n",
    "\n",
    "Can also learn:\n",
    "\n",
    "- English words have common letter patterns\n",
    "- Shakespearean vocabulary and rhythm\n",
    "- How sentences flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import requests\n",
    "\n",
    "torch.manual_seed(42)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Get Shakespeare's Text\n",
    "\n",
    "We'll use the \"tiny Shakespeare\" dataset—a collection of Shakespeare's works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Shakespeare dataset...\n",
      "Download complete!\n",
      "Total characters: 1,115,394\n",
      "\n",
      "First 500 characters:\n",
      "--------------------------------------------------\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor\n"
     ]
    }
   ],
   "source": [
    "# Download Shakespeare dataset\n",
    "if not os.path.exists('shakespeare.txt'):\n",
    "    print(\"Downloading Shakespeare dataset...\")\n",
    "    url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "    response = requests.get(url)\n",
    "    with open('shakespeare.txt', 'w') as f:\n",
    "        f.write(response.text)\n",
    "    print(\"Download complete!\")\n",
    "\n",
    "with open('shakespeare.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(f\"Total characters: {len(text):,}\")\n",
    "print(f\"\\nFirst 500 characters:\")\n",
    "print(\"-\" * 50)\n",
    "print(text[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique characters: 65\n",
      "Characters: \n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n"
     ]
    }
   ],
   "source": [
    "# Look at the character set\n",
    "chars = sorted(set(text))\n",
    "print(f\"Unique characters: {len(chars)}\")\n",
    "print(f\"Characters: {''.join(chars)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Build Vocabulary\n",
    "\n",
    "Notice the vocabulary is different from names:\n",
    "\n",
    "- Uppercase and lowercase letters\n",
    "- Punctuation marks\n",
    "- Newlines and spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 65\n"
     ]
    }
   ],
   "source": [
    "# Create mappings\n",
    "stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "itos = {i: ch for ch, i in stoi.items()}\n",
    "\n",
    "vocab_size = len(stoi)\n",
    "print(f\"Vocabulary size: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Create Training Data\n",
    "\n",
    "Same approach as Part 1, but now we slide a window over continuous text rather than individual words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 1,115,362 examples\n",
      "Input shape: torch.Size([1115362, 32])\n",
      "Target shape: torch.Size([1115362])\n"
     ]
    }
   ],
   "source": [
    "block_size = 32  # Larger context for prose\n",
    "\n",
    "def build_dataset(text, stoi, block_size):\n",
    "    \"\"\"\n",
    "    Create training examples from continuous text.\n",
    "    \n",
    "    Unlike names, we don't have natural boundaries.\n",
    "    We just slide a window over the entire text.\n",
    "    \"\"\"\n",
    "    X, Y = [], []\n",
    "    \n",
    "    for i in range(len(text) - block_size):\n",
    "        context = text[i:i + block_size]\n",
    "        target = text[i + block_size]\n",
    "        \n",
    "        X.append([stoi[ch] for ch in context])\n",
    "        Y.append(stoi[target])\n",
    "    \n",
    "    return torch.tensor(X), torch.tensor(Y)\n",
    "\n",
    "X, Y = build_dataset(text, stoi, block_size)\n",
    "print(f\"Dataset size: {len(X):,} examples\")\n",
    "print(f\"Input shape: {X.shape}\")\n",
    "print(f\"Target shape: {Y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample training examples:\n",
      "------------------------------------------------------------\n",
      "'First Citizen:\n",
      "Before we proceed' → ' '\n",
      "' Citizen:\n",
      "Before we proceed any ' → 'f'\n",
      "'zen:\n",
      "Before we proceed any furth' → 'e'\n"
     ]
    }
   ],
   "source": [
    "# Visualize some examples\n",
    "print(\"Sample training examples:\")\n",
    "print(\"-\" * 60)\n",
    "for i in range(0, 15, 5):\n",
    "    context = ''.join(itos[idx.item()] for idx in X[i])\n",
    "    target = itos[Y[i].item()]\n",
    "    # Show context with target highlighted\n",
    "    print(f\"'{context}' → '{target}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: The Same Model\n",
    "\n",
    "Here's the crucial point: **we use the exact same architecture** from Part 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharLM(nn.Module):\n",
    "    \"\"\"Same architecture as Part 1!\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, block_size, emb_dim, hidden_size):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.hidden = nn.Linear(block_size * emb_dim, hidden_size)\n",
    "        self.output = nn.Linear(hidden_size, vocab_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.emb(x)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = torch.tanh(self.hidden(x))\n",
    "        x = self.output(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model has 149,073 parameters\n"
     ]
    }
   ],
   "source": [
    "# Create model with larger capacity for more complex data\n",
    "emb_dim = 16\n",
    "hidden_size = 256\n",
    "\n",
    "model = CharLM(vocab_size, block_size, emb_dim, hidden_size).to(device)\n",
    "\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Model has {num_params:,} parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Scaling\n",
    "\n",
    "Compared to Part 1:\n",
    "- **Names model**: ~5,000 parameters\n",
    "- **Shakespeare model**: ~50,000+ parameters\n",
    "\n",
    "We scaled up because:\n",
    "1. Larger vocabulary (65 vs 27)\n",
    "2. Longer context (32 vs 5)\n",
    "3. More complex patterns to learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0 | Loss: 2.3469\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 36\u001b[39m\n\u001b[32m     32\u001b[39m             \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m4d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m | Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     34\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m losses\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m losses = \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m4096\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.003\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 22\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(model, X, Y, epochs, batch_size, lr)\u001b[39m\n\u001b[32m     19\u001b[39m loss = loss_fn(logits, y_batch)\n\u001b[32m     21\u001b[39m optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m optimizer.step()\n\u001b[32m     25\u001b[39m total_loss += loss.item()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/base/lib/python3.12/site-packages/torch/_tensor.py:625\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    615\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    616\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    617\u001b[39m         Tensor.backward,\n\u001b[32m    618\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    623\u001b[39m         inputs=inputs,\n\u001b[32m    624\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m625\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    626\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/base/lib/python3.12/site-packages/torch/autograd/__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/base/lib/python3.12/site-packages/torch/autograd/graph.py:841\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    839\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    840\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    842\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    844\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    845\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "def train(model, X, Y, epochs=1000, batch_size=2048, lr=0.001):\n",
    "    model.train()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    \n",
    "    X, Y = X.to(device), Y.to(device)\n",
    "    losses = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        perm = torch.randperm(X.shape[0])\n",
    "        total_loss = 0\n",
    "        n_batches = 0\n",
    "        \n",
    "        for i in range(0, X.shape[0], batch_size):\n",
    "            idx = perm[i:i+batch_size]\n",
    "            x_batch, y_batch = X[idx], Y[idx]\n",
    "            \n",
    "            logits = model(x_batch)\n",
    "            loss = loss_fn(logits, y_batch)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            n_batches += 1\n",
    "        \n",
    "        avg_loss = total_loss / n_batches\n",
    "        losses.append(avg_loss)\n",
    "        \n",
    "        if epoch % 200 == 0:\n",
    "            print(f\"Epoch {epoch:4d} | Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    return losses\n",
    "\n",
    "losses = train(model, X, Y, epochs=1000, batch_size=4096, lr=0.003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Generate Shakespeare!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate(model, itos, stoi, block_size, seed_text=None, length=500, temperature=0.8):\n",
    "    \"\"\"Generate text character by character.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Start with seed text or random text from vocabulary\n",
    "    if seed_text is None:\n",
    "        seed_text = \"ROMEO:\\n\"\n",
    "    \n",
    "    # Pad seed text to block_size\n",
    "    if len(seed_text) < block_size:\n",
    "        seed_text = \" \" * (block_size - len(seed_text)) + seed_text\n",
    "    elif len(seed_text) > block_size:\n",
    "        seed_text = seed_text[-block_size:]\n",
    "    \n",
    "    context = [stoi.get(ch, 0) for ch in seed_text]\n",
    "    generated = list(seed_text)\n",
    "    \n",
    "    for _ in range(length):\n",
    "        x = torch.tensor([context]).to(device)\n",
    "        logits = model(x)\n",
    "        probs = F.softmax(logits / temperature, dim=-1)\n",
    "        next_idx = torch.multinomial(probs, 1).item()\n",
    "        next_char = itos[next_idx]\n",
    "        \n",
    "        generated.append(next_char)\n",
    "        context = context[1:] + [next_idx]\n",
    "    \n",
    "    return ''.join(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"GENERATED SHAKESPEARE (temperature=0.8)\")\n",
    "print(\"=\" * 60)\n",
    "print(generate(model, itos, stoi, block_size, \"ROMEO:\\n\", length=800, temperature=0.8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"GENERATED SHAKESPEARE (temperature=0.5, more focused)\")\n",
    "print(\"=\" * 60)\n",
    "print(generate(model, itos, stoi, block_size, \"To be or not to be\", length=500, temperature=0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Names vs Shakespeare\n",
    "\n",
    "Let's see how the same architecture adapts to different domains:\n",
    "\n",
    "| Aspect | Names Model | Shakespeare Model |\n",
    "|--------|-------------|-------------------|\n",
    "| Vocabulary | 27 (a-z + '.') | 65 (mixed case + punctuation) |\n",
    "| Context | 5 characters | 32 characters |\n",
    "| Pattern type | Word structure | Prose flow |\n",
    "| Output | Single words | Continuous text |\n",
    "| Parameters | ~5K | ~50K |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Universal Pattern\n",
    "\n",
    "Both models learn the same fundamental task:\n",
    "\n",
    "**P(next character | previous characters)**\n",
    "\n",
    "The architecture doesn't change. Only the data and scale differ. This is exactly how large language models work—they're just scaled up versions of what we built here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def plot_embeddings(model, itos, highlight_chars=None):\n",
    "    weights = model.emb.weight.detach().cpu().numpy()\n",
    "    \n",
    "    pca = PCA(n_components=2)\n",
    "    weights_2d = pca.fit_transform(weights)\n",
    "    \n",
    "    plt.figure(figsize=(14, 10))\n",
    "    \n",
    "    for i, (x, y) in enumerate(weights_2d):\n",
    "        char = itos[i]\n",
    "        # Color code by character type\n",
    "        if char.isalpha() and char.isupper():\n",
    "            color = 'blue'\n",
    "        elif char.isalpha() and char.islower():\n",
    "            color = 'green'\n",
    "        elif char.isspace() or char == '\\n':\n",
    "            color = 'red'\n",
    "        else:\n",
    "            color = 'orange'\n",
    "        \n",
    "        plt.scatter(x, y, c=color, s=50)\n",
    "        # Show printable chars, escape others\n",
    "        label = char if char.isprintable() and not char.isspace() else repr(char)[1:-1]\n",
    "        plt.annotate(label, (x, y), fontsize=10)\n",
    "    \n",
    "    plt.title('Shakespeare Character Embeddings')\n",
    "    plt.xlabel('PC1')\n",
    "    plt.ylabel('PC2')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    from matplotlib.patches import Patch\n",
    "    legend_elements = [\n",
    "        Patch(facecolor='blue', label='Uppercase'),\n",
    "        Patch(facecolor='green', label='Lowercase'),\n",
    "        Patch(facecolor='red', label='Whitespace'),\n",
    "        Patch(facecolor='orange', label='Punctuation')\n",
    "    ]\n",
    "    plt.legend(handles=legend_elements)\n",
    "    plt.show()\n",
    "\n",
    "plot_embeddings(model, itos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Limitation: Character-Level\n",
    "\n",
    "Our model works, but it has a fundamental limitation: **it sees characters, not words**.\n",
    "\n",
    "When predicting the next character after \"The king\", our model must:\n",
    "1. Remember that 'T-h-e- -k-i-n-g' was seen\n",
    "2. Predict what comes next one character at a time\n",
    "\n",
    "This is inefficient. Consider:\n",
    "- To learn \"the\", it needs \"t→h\", \"h→e\", \"e→ \" patterns\n",
    "- To learn \"thee\" (Shakespearean), it needs different patterns\n",
    "- No concept of words as units"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preview of Part 3\n",
    "\n",
    "In Part 3, we'll introduce **Byte-Pair Encoding (BPE)**, which learns to tokenize at the subword level:\n",
    "\n",
    "- Common words become single tokens: \"the\" → [token_123]\n",
    "- Rare words split into pieces: \"shakespeare\" → [\"shake\", \"speare\"]\n",
    "\n",
    "This is how GPT-2, GPT-3, and modern LLMs tokenize text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "We demonstrated that our character-level language model is **domain-agnostic**:\n",
    "\n",
    "1. Same architecture works on names and Shakespeare\n",
    "2. Only data and hyperparameters change\n",
    "3. The model learns whatever patterns exist in the data\n",
    "\n",
    "This is the power of neural language models—they're universal pattern learners."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's Next\n",
    "\n",
    "In **Part 3**, we'll build a **BPE tokenizer from scratch** and show how subword tokenization improves model quality and efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. **Different prompts**: Try generating from \"HAMLET:\", \"JULIET:\", \"First Citizen:\"\n",
    "2. **Context length**: What happens with block_size = 8 vs 64?\n",
    "3. **Temperature exploration**: Generate at temperatures 0.2, 0.5, 1.0, 1.5\n",
    "4. **Train longer**: What's the minimum loss you can achieve?\n",
    "5. **Measure perplexity**: Compute exp(loss) on held-out text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

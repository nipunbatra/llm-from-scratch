{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Self-Attention from Scratch\n",
    "## The heart of the transformer\n",
    "\n",
    "In Parts 1-3, our model processed context like this:\n",
    "\n",
    "```\n",
    "Input: [token_1, token_2, token_3, token_4, token_5]\n",
    "         ↓        ↓        ↓        ↓        ↓\n",
    "      [emb_1,   emb_2,   emb_3,   emb_4,   emb_5]\n",
    "         ↓        ↓        ↓        ↓        ↓\n",
    "      [              FLATTEN                    ]\n",
    "         ↓\n",
    "      [         SINGLE HIDDEN LAYER            ]\n",
    "         ↓\n",
    "      [              OUTPUT                     ]\n",
    "```\n",
    "\n",
    "**The problem**: Every position is treated identically. The model can't dynamically focus on relevant tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enter Attention\n",
    "\n",
    "Attention lets each position **look at and weight other positions** based on relevance:\n",
    "\n",
    "```\n",
    "\"The cat sat on the mat\"\n",
    "          ↓\n",
    "When predicting after \"mat\", attention might:\n",
    "- Focus heavily on \"cat\" (the subject)\n",
    "- Focus on \"sat\" (the verb)\n",
    "- Ignore \"the\" (low information)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import os\n",
    "import requests\n",
    "\n",
    "torch.manual_seed(42)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "if not os.path.exists('shakespeare.txt'):\n",
    "    url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "    response = requests.get(url)\n",
    "    with open('shakespeare.txt', 'w') as f:\n",
    "        f.write(response.text)\n",
    "\n",
    "with open('shakespeare.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "chars = sorted(set(text))\n",
    "stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "itos = {i: ch for ch, i in stoi.items()}\n",
    "vocab_size = len(stoi)\n",
    "print(f\"Vocabulary size: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Understanding Attention Mathematically\n",
    "\n",
    "Attention computes a weighted combination of values, where weights are based on query-key similarity.\n",
    "\n",
    "**Formula**:\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right) V$$\n",
    "\n",
    "Where:\n",
    "- **Q** (Query): What am I looking for?\n",
    "- **K** (Key): What do I contain?\n",
    "- **V** (Value): What information do I provide?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_attention(query, key, value):\n",
    "    \"\"\"\n",
    "    Basic attention mechanism.\n",
    "    \n",
    "    Args:\n",
    "        query: [batch, seq_len, d_k]\n",
    "        key: [batch, seq_len, d_k]\n",
    "        value: [batch, seq_len, d_v]\n",
    "    \n",
    "    Returns:\n",
    "        output: [batch, seq_len, d_v]\n",
    "        attention_weights: [batch, seq_len, seq_len]\n",
    "    \"\"\"\n",
    "    d_k = query.shape[-1]\n",
    "    \n",
    "    # Step 1: Compute attention scores (how much each position attends to each other)\n",
    "    # [batch, seq_len, d_k] @ [batch, d_k, seq_len] = [batch, seq_len, seq_len]\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1))\n",
    "    \n",
    "    # Step 2: Scale by sqrt(d_k) to prevent softmax saturation\n",
    "    scores = scores / math.sqrt(d_k)\n",
    "    \n",
    "    # Step 3: Softmax to get attention weights (sum to 1)\n",
    "    attention_weights = F.softmax(scores, dim=-1)\n",
    "    \n",
    "    # Step 4: Weighted sum of values\n",
    "    output = torch.matmul(attention_weights, value)\n",
    "    \n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: 3 tokens, embedding dim 4\n",
    "seq_len = 3\n",
    "d_model = 4\n",
    "\n",
    "# Random embeddings (pretend these represent \"The cat sat\")\n",
    "embeddings = torch.randn(1, seq_len, d_model)\n",
    "\n",
    "# In self-attention, Q, K, V all come from the same input\n",
    "Q = embeddings\n",
    "K = embeddings\n",
    "V = embeddings\n",
    "\n",
    "output, weights = simple_attention(Q, K, V)\n",
    "\n",
    "print(f\"Input shape: {embeddings.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"\\nAttention weights (who attends to whom):\")\n",
    "print(weights.squeeze().numpy().round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize attention weights\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.imshow(weights.squeeze().numpy(), cmap='Blues')\n",
    "plt.colorbar(label='Attention weight')\n",
    "plt.xlabel('Key position (attending TO)')\n",
    "plt.ylabel('Query position (attending FROM)')\n",
    "plt.title('Attention Weights')\n",
    "positions = ['Pos 0', 'Pos 1', 'Pos 2']\n",
    "plt.xticks(range(3), positions)\n",
    "plt.yticks(range(3), positions)\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        plt.text(j, i, f'{weights[0, i, j]:.2f}', ha='center', va='center')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Causal (Masked) Attention\n",
    "\n",
    "For language modeling, we can only attend to **past** positions (can't peek at future tokens!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def causal_attention(query, key, value):\n",
    "    \"\"\"\n",
    "    Causal (masked) attention - can only attend to past positions.\n",
    "    \"\"\"\n",
    "    d_k = query.shape[-1]\n",
    "    seq_len = query.shape[1]\n",
    "    \n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "    \n",
    "    # Create causal mask: upper triangle = -inf (can't attend to future)\n",
    "    mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool()\n",
    "    scores = scores.masked_fill(mask, float('-inf'))\n",
    "    \n",
    "    attention_weights = F.softmax(scores, dim=-1)\n",
    "    output = torch.matmul(attention_weights, value)\n",
    "    \n",
    "    return output, attention_weights\n",
    "\n",
    "# Test causal attention\n",
    "output_causal, weights_causal = causal_attention(Q, K, V)\n",
    "print(\"Causal attention weights:\")\n",
    "print(weights_causal.squeeze().numpy().round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize causal mask\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.imshow(weights_causal.squeeze().numpy(), cmap='Blues')\n",
    "plt.colorbar(label='Attention weight')\n",
    "plt.xlabel('Key position (can attend TO)')\n",
    "plt.ylabel('Query position (attending FROM)')\n",
    "plt.title('Causal Attention Weights (masked future)')\n",
    "plt.xticks(range(3), ['Pos 0', 'Pos 1', 'Pos 2'])\n",
    "plt.yticks(range(3), ['Pos 0', 'Pos 1', 'Pos 2'])\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        plt.text(j, i, f'{weights_causal[0, i, j]:.2f}', ha='center', va='center')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice:\n",
    "- Position 0 can only attend to itself (weight = 1.0)\n",
    "- Position 1 attends to positions 0 and 1\n",
    "- Position 2 attends to all positions 0, 1, 2\n",
    "- No position attends to the future!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Self-Attention Layer\n",
    "\n",
    "In practice, we learn separate projections for Q, K, V:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Self-attention layer with learned projections.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, d_k=None):\n",
    "        super().__init__()\n",
    "        if d_k is None:\n",
    "            d_k = d_model\n",
    "        \n",
    "        # Learned projections\n",
    "        self.W_q = nn.Linear(d_model, d_k, bias=False)\n",
    "        self.W_k = nn.Linear(d_model, d_k, bias=False)\n",
    "        self.W_v = nn.Linear(d_model, d_k, bias=False)\n",
    "        self.d_k = d_k\n",
    "    \n",
    "    def forward(self, x, mask=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: [batch, seq_len, d_model]\n",
    "            mask: Whether to apply causal mask\n",
    "        \n",
    "        Returns:\n",
    "            output: [batch, seq_len, d_k]\n",
    "        \"\"\"\n",
    "        # Project to Q, K, V\n",
    "        Q = self.W_q(x)\n",
    "        K = self.W_k(x)\n",
    "        V = self.W_v(x)\n",
    "        \n",
    "        # Compute attention scores\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        \n",
    "        # Apply causal mask if needed\n",
    "        if mask:\n",
    "            seq_len = x.shape[1]\n",
    "            causal_mask = torch.triu(torch.ones(seq_len, seq_len, device=x.device), diagonal=1).bool()\n",
    "            scores = scores.masked_fill(causal_mask, float('-inf'))\n",
    "        \n",
    "        # Softmax and weighted sum\n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        output = torch.matmul(attention_weights, V)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Multi-Head Attention\n",
    "\n",
    "Instead of one attention, we compute multiple \"heads\" in parallel. Each head can learn different patterns:\n",
    "\n",
    "- Head 1: Learn syntactic relationships\n",
    "- Head 2: Learn semantic relationships\n",
    "- Head 3: Learn positional patterns\n",
    "- etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-head attention: multiple attention heads in parallel.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, n_heads):\n",
    "        super().__init__()\n",
    "        assert d_model % n_heads == 0\n",
    "        \n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = d_model // n_heads\n",
    "        \n",
    "        # Combined projections for efficiency\n",
    "        self.W_qkv = nn.Linear(d_model, 3 * d_model, bias=False)\n",
    "        self.W_out = nn.Linear(d_model, d_model, bias=False)\n",
    "    \n",
    "    def forward(self, x, mask=True):\n",
    "        batch, seq_len, d_model = x.shape\n",
    "        \n",
    "        # Project and split into Q, K, V\n",
    "        qkv = self.W_qkv(x)  # [batch, seq_len, 3 * d_model]\n",
    "        qkv = qkv.reshape(batch, seq_len, 3, self.n_heads, self.d_k)\n",
    "        qkv = qkv.permute(2, 0, 3, 1, 4)  # [3, batch, n_heads, seq_len, d_k]\n",
    "        Q, K, V = qkv[0], qkv[1], qkv[2]\n",
    "        \n",
    "        # Attention scores\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        \n",
    "        # Causal mask\n",
    "        if mask:\n",
    "            causal_mask = torch.triu(torch.ones(seq_len, seq_len, device=x.device), diagonal=1).bool()\n",
    "            scores = scores.masked_fill(causal_mask, float('-inf'))\n",
    "        \n",
    "        attention = F.softmax(scores, dim=-1)\n",
    "        output = torch.matmul(attention, V)\n",
    "        \n",
    "        # Combine heads\n",
    "        output = output.permute(0, 2, 1, 3).reshape(batch, seq_len, d_model)\n",
    "        output = self.W_out(output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Positional Encoding\n",
    "\n",
    "Attention is permutation-invariant—it doesn't know position! We add positional information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Sinusoidal positional encoding (from original Transformer paper).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Create positional encodings\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        # Register as buffer (not a parameter)\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.shape[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize positional encodings\n",
    "pe = PositionalEncoding(64, 100)\n",
    "encodings = pe.pe.squeeze().numpy()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.imshow(encodings[:50, :32].T, aspect='auto', cmap='RdBu')\n",
    "plt.colorbar()\n",
    "plt.xlabel('Position in sequence')\n",
    "plt.ylabel('Encoding dimension')\n",
    "plt.title('Positional Encodings (first 50 positions, 32 dimensions)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Transformer Block\n",
    "\n",
    "A complete transformer block combines:\n",
    "1. Multi-head attention\n",
    "2. Feed-forward network\n",
    "3. Residual connections\n",
    "4. Layer normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A single transformer block.\n",
    "    \n",
    "    Architecture:\n",
    "        x → LayerNorm → MultiHeadAttention → + (residual)\n",
    "        → LayerNorm → FFN → + (residual)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, n_heads, d_ff=None, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        if d_ff is None:\n",
    "            d_ff = 4 * d_model\n",
    "        \n",
    "        self.attention = MultiHeadAttention(d_model, n_heads)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(d_ff, d_model),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Attention with residual\n",
    "        x = x + self.dropout(self.attention(self.ln1(x)))\n",
    "        \n",
    "        # FFN with residual\n",
    "        x = x + self.ffn(self.ln2(x))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Complete Transformer Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerLM(nn.Module):\n",
    "    \"\"\"\n",
    "    A small transformer language model.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, d_model, n_heads, n_layers, block_size, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.token_emb = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_enc = PositionalEncoding(d_model, block_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(d_model, n_heads, dropout=dropout)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "        \n",
    "        self.ln_final = nn.LayerNorm(d_model)\n",
    "        self.output = nn.Linear(d_model, vocab_size)\n",
    "        \n",
    "        self.block_size = block_size\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Token embeddings + positional encoding\n",
    "        x = self.token_emb(x)\n",
    "        x = self.pos_enc(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Transformer blocks\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        \n",
    "        # Output projection\n",
    "        x = self.ln_final(x)\n",
    "        logits = self.output(x)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create model with increased capacity\nblock_size = 128  # Increased from 64\nd_model = 256     # Increased from 128\nn_heads = 8       # Increased from 4\nn_layers = 6      # Increased from 4\n\nmodel = TransformerLM(\n    vocab_size=vocab_size,\n    d_model=d_model,\n    n_heads=n_heads,\n    n_layers=n_layers,\n    block_size=block_size,\n    dropout=0.1\n).to(device)\n\nnum_params = sum(p.numel() for p in model.parameters())\nprint(f\"Model parameters: {num_params:,}\")\nprint(f\"\\nArchitecture:\")\nprint(f\"  Embedding dim: {d_model}\")\nprint(f\"  Attention heads: {n_heads}\")\nprint(f\"  Layers: {n_layers}\")\nprint(f\"  Block size: {block_size}\")\n\n# Model scale context\nprint(f\"\\n--- Model Scale Context ---\")\nprint(f\"Our model:     ~{num_params/1_000_000:.1f}M parameters\")\nprint(f\"GPT-2 Small:   124M parameters  ({124_000_000/num_params:.0f}x larger)\")\nprint(f\"GPT-3:         175B parameters  ({175_000_000_000/num_params:.0f}x larger)\")\nprint(f\"Claude/GPT-4:  ~1T+ parameters  ({1_000_000_000_000/num_params:.0f}x larger)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "def build_dataset(text, block_size, stoi):\n",
    "    data = [stoi[ch] for ch in text]\n",
    "    X, Y = [], []\n",
    "    for i in range(len(data) - block_size):\n",
    "        X.append(data[i:i + block_size])\n",
    "        Y.append(data[i + 1:i + block_size + 1])\n",
    "    return torch.tensor(X), torch.tensor(Y)\n",
    "\n",
    "X, Y = build_dataset(text, block_size, stoi)\n",
    "print(f\"Dataset: {len(X):,} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, X, Y, epochs=500, batch_size=64, lr=3e-4):\n",
    "    model.train()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    \n",
    "    X, Y = X.to(device), Y.to(device)\n",
    "    losses = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        perm = torch.randperm(X.shape[0])\n",
    "        total_loss, n_batches = 0, 0\n",
    "        \n",
    "        for i in range(0, len(X), batch_size):\n",
    "            idx = perm[i:i+batch_size]\n",
    "            x_batch, y_batch = X[idx], Y[idx]\n",
    "            \n",
    "            logits = model(x_batch)\n",
    "            # Compute loss for all positions\n",
    "            loss = F.cross_entropy(logits.view(-1, vocab_size), y_batch.view(-1))\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            n_batches += 1\n",
    "        \n",
    "        losses.append(total_loss / n_batches)\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch}: Loss = {losses[-1]:.4f}\")\n",
    "    \n",
    "    return losses\n",
    "\n",
    "losses = train(model, X, Y, epochs=500, batch_size=64, lr=3e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Transformer Training Loss')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Generate Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate(model, seed_text, max_len=500, temperature=0.8):\n",
    "    model.eval()\n",
    "    \n",
    "    tokens = [stoi[ch] for ch in seed_text]\n",
    "    generated = list(seed_text)\n",
    "    \n",
    "    for _ in range(max_len):\n",
    "        # Take last block_size tokens\n",
    "        context = tokens[-block_size:] if len(tokens) >= block_size else tokens\n",
    "        x = torch.tensor([context]).to(device)\n",
    "        \n",
    "        logits = model(x)\n",
    "        # Get prediction for last position\n",
    "        logits = logits[0, -1, :] / temperature\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        next_idx = torch.multinomial(probs, 1).item()\n",
    "        \n",
    "        tokens.append(next_idx)\n",
    "        generated.append(itos[next_idx])\n",
    "    \n",
    "    return ''.join(generated)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"GENERATED TEXT (Transformer)\")\n",
    "print(\"=\" * 60)\n",
    "print(generate(model, \"ROMEO:\\n\", max_len=500))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Save Model for Later Use\n\nExport the transformer model for use in Part 5 (instruction tuning) and beyond.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import os\n\n# Create models directory\nos.makedirs('../models', exist_ok=True)\n\n# Save model checkpoint with architecture info\ncheckpoint = {\n    'model_state_dict': model.state_dict(),\n    'model_config': {\n        'vocab_size': vocab_size,\n        'd_model': d_model,\n        'n_heads': n_heads,\n        'n_layers': n_layers,\n        'block_size': block_size,\n    },\n    'stoi': stoi,\n    'itos': itos,\n    'final_loss': losses[-1] if losses else None,\n}\n\ntorch.save(checkpoint, '../models/transformer_shakespeare.pt')\nprint(f\"Model saved to ../models/transformer_shakespeare.pt\")\n\n# Verify\nloaded = torch.load('../models/transformer_shakespeare.pt', weights_only=False)\nprint(f\"Model config: {loaded['model_config']}\")\nprint(f\"Model parameters: {num_params:,}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Clean Up GPU Memory",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import gc\n\n# Delete model and data tensors\ndel model\ndel X, Y\n\n# Clear CUDA cache\nif torch.cuda.is_available():\n    torch.cuda.empty_cache()\n    torch.cuda.synchronize()\n\ngc.collect()\n\nprint(\"GPU memory cleared!\")\nif torch.cuda.is_available():\n    print(f\"GPU memory allocated: {torch.cuda.memory_allocated() / 1024**2:.1f} MB\")\n    print(f\"GPU memory cached: {torch.cuda.memory_reserved() / 1024**2:.1f} MB\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing MLP vs Transformer\n",
    "\n",
    "| Aspect | MLP (Part 1-3) | Transformer (Part 4) |\n",
    "|--------|----------------|----------------------|\n",
    "| Context processing | Flatten + FC | Self-attention |\n",
    "| Position awareness | None (implicit) | Explicit encoding |\n",
    "| Dynamic focus | No | Yes (attention) |\n",
    "| Parallelization | Full | Full |\n",
    "| Learns long-range | Difficult | Natural |\n",
    "| Parameters (our models) | ~50K | ~400K |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "We built self-attention from scratch:\n",
    "\n",
    "| Component | Purpose |\n",
    "|-----------|---------|\n",
    "| Q, K, V projections | Learn what to look for/store |\n",
    "| Attention scores | Measure relevance between positions |\n",
    "| Causal mask | Prevent looking at future |\n",
    "| Multi-head | Multiple attention patterns |\n",
    "| Positional encoding | Tell model about position |\n",
    "| Transformer block | Attention + FFN + residuals |\n",
    "\n",
    "This is the architecture behind GPT-2, GPT-3, GPT-4, Claude, and all modern LLMs!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's Next\n",
    "\n",
    "In **Part 5**, we'll take our pretrained model and **instruction-tune** it to follow instructions. We'll teach it to answer questions rather than just complete text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. **Attention visualization**: Plot attention patterns for a generated sequence\n",
    "2. **Head analysis**: What does each attention head learn?\n",
    "3. **Scale up**: Increase layers/heads. How does quality change?\n",
    "4. **Learned positions**: Replace sinusoidal with learned positional embeddings\n",
    "5. **RoPE/ALiBi**: Implement modern positional encoding methods"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nb-base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}